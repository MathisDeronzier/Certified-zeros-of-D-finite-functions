
\documentclass[a4paper,10.5pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[utf8]{inputenc}
\textwidth15cm
\oddsidemargin+0.25cm
\textheight24cm
\topmargin-2cm

\title{Zéros certifiés des fonctions D-finies}
\author{Mathis Deronzier}
\date{}

\begin{document}
	
	\maketitle
	\renewcommand{\contentsname}{Sommaire}
	\newpage
	\tableofcontents
	\newpage
	
	\section{Introduction}
	\newtheorem{theorem}{Théorème}[section] 
	\newtheorem{proposition}{Proposition}
	\newtheorem{corollaire}{Corollaire}
	\newtheorem{definition}{Définition}
	\newtheorem{demonstration}{Démonstration}
	\subsection{Remerciments}
	\subsection{Cadre}
	\section{Fonctions D-finies}
	
	Une fonction est dite  D-finies si elle satisfait une équation différentielles de la forme
	\begin{equation}
		a_{r}(z)y^{(r)}(z)+a_{n-1}(z)y^{(n-1)}(z)+...+a_{0}(z)y(z)=0, \hspace{3mm} a_{k}\in \mathbb{C}[z].
	\end{equation}
 
	
	
	
	
	
	\section{Approximation uniforme sur un segment}
	
	Dans cette section, on approchera les fonctions D-finies à l'aide des polynômes.
	Weierstrass a montré qu'il était possible d'approximer sur un segment n'importe quelle fonction continue à partir de polynômes. Cependant, toutes les méthodes d'interpolation ne convergent pas. Un exemple est donné par Runge, la suite de polynômes qui interpolent la fonction $f(x)=\frac{1}{1+x^{2}}$ sur $[-5,5]$ sur N points à distance régulière les uns des autres diverge lorsque $N \rightarrow \infty$.
	
	\subsection{Polynômes de Chebyshev méthode de Boyd}
	
	
	Le mathématicien Russe Sergei Bernstein (1880-1968) a montré que la série de Chebyshev d'une fonction analytique a une convergence quadratique par rapport à la norme infinie. C'est à dire que l'erreur, après avoir tronqué la série après le Nème terme, est $O(exp(-N\mu))$. Dans notre cadre, il est intéressant de vouloir quantifier avec des bornes concrêtes notre approximation.
	C'est cette majoration avec le théorème des valeurs intermédiaires qui nous permettra de certifier les zéros de notre fonction D-finie f.
	
	
	\begin{proposition}
		Soit f une fonction continue, soit g une fonction telle que \\
		$\left\|f-g \right\|_{\infty} \leq \epsilon$. Alors, si il existe a et b tels que $\left\|g(a)\right\| \geq \epsilon$ et $\left\|g(b)\right\| \geq \epsilon$ avec $g(a)g(b) < 0$, alors f s'annule sur l'intervalle [a,b].
		
	\end{proposition}
	
	\subsection{L'algorithme d'approximation}
	
	
	\begin{algorithm}
		\caption{Chebyshev approximation}
		
		\vspace{2mm}
		
		\textbf{Entrée:} fonction D-finie $f(x)=\sum_{n \geq\beta} f(n)x^{n}$. Entier N
		
		\textbf{Sortie:} polynôme d'interpolation g
		
		\begin{algorithmic}[1]
			
			\STATE création des points d'interpolation:$x_{k}=\frac{b-a}{2}cos(\pi\frac{k}{N})+\frac{b+a}{2}$  $k=0,1,..,N$
			\STATE création des points à approximer: $f_{k}=f(x_{k})$ $k=0,1,..,N$
			\STATE création de la matrice d'interpolation M de taille $(N+1)\times (N+1)$:\\
			$p_{j}=2$ $j\in\{1,2\}$ et $p_{j}=1$ sinon, alors:
			$M_{jk}=\frac{2}{p_{j}p_{k}N}cos(j\pi\frac{k}{N})$
			\STATE $a_{j}=\sum_{k=0}^{N}M_{jk}f_{k}$ j=0,1,..,N
			\STATE $g(x)=\sum_{j=0}^{N}a_{j}T_{j}(\frac{2x-(b+a)}{b-a})$
			\STATE Renvoyer g
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent $T_{j}(x)=cos(j$  $arccos(x))$
	
	\noindent On voit ici la limite de cet algorithme, il nécessite une connaissance de la fonction, et c'est justement ce que nous cherchons ici.
	
	\subsection{La méthode des éléments finis}
	
	Une bonne méthode de résolution d'équation différentielle est la méthode des éléments finis. Il faut réfléchir à une famille de fonctions qu'il pourrait-être intéressant de prendre ici. La famille usuelle des fonctions en "escalier" ne semble pas suffisante puisqu'elle ne satisfait la condition $C^{\infty}$.
	La famille des polynômes de Chebyshev semble donc la solution.
	
	\subsection{Méthode de Sturm}
	
	Sur un intervalle donné, on veut maintenant voir s'il y a des zéros grâce à l'approximation de Chebyshev et la proposition 1. Nous allons utiliser le principe d'exclusion en comptant le nombre de racines dans l'intervalle à l'aide du théorème de Sturm.
	
	
	Soit P un polynôme unitaire, $P(x)=x^{n} + \sum^{n-1}_{k=0}a_{k}x^{k}$, \textit {la suite de Sturm} est une suite finie de polynômes définie à partir de P comme suit:\\
	$P_{0}=P$, $P_{1}=P'$, et pour $k > 1$ si $P_{k} \neq 0$,  $P_{k+1}$ vérifie
	\[P_{k-1}=P_{k}Q_{k}-P_{k+1},\hspace{2mm}\text{avec }    deg(P_{k+1})<deg(P_{k})\] 
	$P_{k+1}$ est l'opposée du reste dans la division euclidienne de $P_{k-1}$ par $P_{k}$. On a alors le théorème suivant 
	

	\begin{theorem}(Sturm-Habicht)
		Notons $\sigma(\xi)$ le nombre de fois où la suite $P(\xi),P_{1}(\xi),...,P_{m}(\xi)$ change de signe (un zéro ne compte pas comme changement de signe).
		Pour deux réels $a,b$ avec $a<b$ où $a$ et $b$ ne sont pas des racines de P, le nombre de racines dans l'intervalle $[a,b]$ vaut
		$\sigma(a)-\sigma(b)$.
	\end{theorem}
	
	\section{Méthode de Newton}
	
	La méthode de Newton définie par la suite $x_{k+1}=N_{f}(x_{k})$ avec $N_{f}(x)=x-f'(x)^{-1}f(x)$ dans notre cas, nous sommes dans $\mathbb{R}$ alors la méthode de Newton se réécrit $N_{f}(x)=x-\frac{f(x)}{f'(x)}$ 
	
	\subsection{Les applications contractantes}
	
	Ce chapitre se compose de quelques rappels sur les applications contractantes, qui sont la clés de la convergence de la méthode de Newton. En effet, la convergence de cette méthode est assurée lorsque celle-ci est contractancte. 
	Notons $\mathbb{E}$ un espace métrique complet et $d$ sa distance.
	
	
	\begin{definition} Une application $f: \mathbb{E} \rightarrow \mathbb{E}$ est lipschitzienne s'il existe $\lambda \in \mathbb{R}_{+}$ tel que pour tout $x$ et $y in\ \mathbb{E}$ on ait $d(f(x),f'(y)) \leq \lambda$. Une application est dite contractante si elle est lipshitzienne pour une constante $\lambda <1$. 
	\end{definition}
	On peut par ailleurs définir une constante de Lipschitz d'une fonction f sur un segment $[a,b]$
	\[Lip(f)=\sup_{x \neq y,(x,y)\in [a,b]^2} \frac{d(f(x),f(y))}{d(x,y)}\]
	
	On va maintenant donner les théorèmes qui nous seront utiles.
	\begin{theorem}(Théorème des applications conctractantes) Soit $f:\mathbb{E} \rightarrow \mathbb{E}$ une application contractante de constante $0<\lambda<1$.
		
		(a)Pour tout $x_0 \in \mathbb{E}$, la suite $x_{k+1}=f(x_k)$ converge vers un point fixe,\\
		
		(b)Ce point fixe est unique et on le nomme $x$,\\
		
		(c)Pour tout $q \geq 0$, $d(x_q,x) \leq \frac{\lambda^q}{1-\lambda} d(x_0,x_1)$,\\
		
		(d)$\frac{d(x_0,x_1)}{1+\lambda} \leq d(x_0,x) \leq \frac{d(x_0,x_1)}{1-\lambda}$.
	\end{theorem}
	Ce théorème est bien utile, la dernière inégalité permet d'encadrer la distance au zéro, bien utile pour trouver un zéro à une précision $epsilon$. 
	
	\subsection{Comment vérifier les hypothèses de contraction} 
	
	\subsection{L'algorithme de Newton}
	
	
	
	\begin{algorithm}
		\caption{Newton iteration}
		
		\vspace{2mm}
		
		\textbf{Entrée:} fonction $f(x)$. rationnel a. $\epsilon$ une distance. $\lambda$ la constante de Lipschitz.
		
		\textbf{Sortie:} point b à une distance inférieur à epsilon au zéro le plus proche. 
		
		\begin{algorithmic}[1]
			\STATE x:=$N_{f}(a)$
			\STATE Tant que $d(x,N_{f}(x))>\frac{\epsilon}{1+\lambda}$\\
					x:=$N_{f}(x)$					
			\STATE return $N_{f}(x)$
		\end{algorithmic}
	\end{algorithm}

	\subsection{Théorie alpha de Smale}
	La théorie alpha de Smale permet d'assurer la présence d'un zéro dans un voisinage, autrement dit de certifier les zéros, les principaux résultats de cette théorie sont rappelés dans la prochaine section.
	Dans cette section les espaces considérés sont des espaces de Banach, $U$ est un ouvert de l'espace. Et les fonctions $f:U\rightarrow \mathbb{F}$ sont analytiques sur U. Nous sommes dans le cas particulier de $\mathbb{R}$ mais cette théorie s'applique sur tout espace métrique complet. 
	
	\begin{definition}
		
		On définit les trois opérateurs $\gamma(f,x)$, $\beta(f,x)$, et $\alpha(f,x)$ sur les fonctions analytiques sur $\mathbb{R}$:
		
		\[\gamma(f,x)=\sup_{k \geq 2}\left\|f'(x)^{-1}\frac{f^{(k)}(x)}{k!}\right\|^{\frac{1}{k-1}}\]
		\[\beta(f,x)=\left\|f'(x)^{-1}f(x)\right\|\]
		\[\alpha(f,x)=\gamma(f,x)\beta(f,x)\]
	\end{definition}
	
	\begin{theorem}(Théorème gamma) Soit $\zeta \in U$ tel que $f(\zeta)=0$ et $f'(\zeta)$ soient inversibles. Soit $x_{0} \in U$ tel que \\
		\[\left\|x_{0}-\zeta\right\|\gamma(f,\zeta) \leq \frac{3-\sqrt{7}}{2}=0.17712.... \]
		Alors la suite de Newton $x_{k+1}=N_{f}(x_{k})$ converge vers $\zeta$. De plus\\
		\[\left\|x_{k}-\zeta\right\| \leq \left(\frac{1}{2}\right)^{n}\left\|x_{0}-\zeta\right\|\]
	\end{theorem}
	
	
	\begin{theorem} (Wang-Han)
		Pour tout $\alpha$ $\in [0,3-2\sqrt{2}]$, la quantité $(1+\alpha^{2})-8\alpha$ décroît de 0 à 1. Posons
		
		\[q=\frac{1-\alpha-\sqrt{(1+\alpha)^{2}-8\alpha}}{1-\alpha+\sqrt{(1+\alpha)^{2}-8\alpha}}\]
		
		On a
		\[0 \leq q<1 \text{ si } 0 \leq a < 3-2\sqrt{2}\]
		\[q=1        \text{ si } 0 \leq a < 3-2\sqrt{2}\]
		Pour tout $x\in U$ tel que $\alpha=\alpha(f,x) \leq 3-2\sqrt{2}$, il existe un et un seul zéro $\zeta$ de $f$ tel que\\
	\end{theorem}
	
	
	\begin{corollaire}
		Pour tout $x\in U$ tel que $\alpha=\alpha(f,x) \leq 3-2\sqrt{2}$, il existe un et un seul zéro $\zeta$ de f tel que
		\[\left\|\zeta-x\right\|\leq\frac{2-\sqrt{2}}{2\gamma(f,x)}\]
		De plus, la suite de Newton $x_{k+1}=N_{f}(x_{k}),$ $x_{0}=x$, est définie et converge vers $\zeta$.
	\end{corollaire}
	\noindent \textbf{Esquisse de démonstration} $(2-\sqrt(2)/2$ est le maximum de $(1+\alpha-\sqrt{(1+\alpha)^{2}-8\alpha})/4$ lorsque $\alpha \in [0,3-2\sqrt(2)]$.
	
	(À compléter)
	\noindent Opérateur $\gamma(f,x)$
	
	Ce sera ce dernier théorème qui nous permettra de vérifier si nous avons bien un zéro. Cependant le calcul de $\gamma$ et donc de $\alpha$ est encore difficile pour des coefficients de fonction D-finies vérifiant une récurrence polynomiale. Le chapitre sur les séries majorantes fournie une majoration du coefficient $\alpha$
	
	
	
	
	\section{Majoration des suites P-récursives}
	
	Ce chapitre est quasiment une réécriture du chapitre 5 de la thèse de Marc Mezzarobba. Il majore les suites P-récursives à l'aide du théorème de Perron-Kreuser.
	L'avantage de cette majoration est qu'elle sera fine, c'est à dire qu'on peut quantifier la différence entre la majoration et la solution.
	Cette majoration terme à terme permettra donc de trouver une majoration fine du coefficient alpha et donc des bassins d'attractions donnés par le théorème de (Wang-Han), ce qui permettra à notre algorithme de certifier ses zéros.
	
	Considérons une suite P-récursive u, définie par la récurrence
	\[p^{[0]}(n+s)u_{n}+p^{[1]}(n)u_{n+s-1}+...+p^{[s]}(n)u_{n}=0,\hspace{2mm} p^{[k]}\in \mathbb{Q} \tag{*}\] 
	\begin{theorem}Soit $(u_{n})\in \mathbb{Q}^{\mathbb{N}}$ une suite P-récursive solution de la récurrence homogène (*), avec $p^{[s]}(n) \neq 0$ et $p^{[0]}(n) \neq 0$ pour $n \in \mathbb{N}$. Étant donnée la récurrence (*) et les conditions initiales $u_{0},...,u_{s-1}$, l'algorithme défini dans cette section calcule un réel positif A, un rationnel $\kappa$, un nombre algébrique $\alpha$  et une fonction $\phi$ telle que
		\[\forall n\in \mathbb{N}, \hspace{6mm}|u_{n}| \leq A n!^{\kappa}\alpha^{n}\phi(n)\]
		Avec $\phi(n)=e^{o(n)}$. Pour choix générique des conditions initiales, les paramètres $\kappa$ et $\alpha$ sont optimaux.
	\end{theorem}
	
	La fonction $\phi$ est donnée par une formule explicite, elle-même décrite par un petit nombre de paramètres. Les formes qu'elle peut prendre sont détaillées par la suite.
	
	\subsection{Le théorème de Perron-Kreuser}
	
	On s'intéresse ici au comportement asymptotique des solutions des récurrences. Supposons que les coefficients $b_{n}(n)$ de l'équation
	\[b_{s}(n)u_{n+s}+b_{s-1}(n)u_{n+s-1}+...+b_{s'}(n)u_{n+s'}=0  \tag{*}\]
	(où s' peut être négatif) ont des comportements asymptotiques de la forme
	\[\forall k, \hspace{2mm} b_{k}(n) \sim c_{k}n^{d_{k}} \hspace{2mm} \text{quand } n \rightarrow \infty\]
	avec $c_{k} \in E$ et $d_{k} \in \mathbb{Z}$. Supposons de plus que $u_{n}$ est une solution de la forme  
	\[\frac{u_{n+1}}{u_{n}}\sim \lambda n^{\kappa} \hspace{2mm} \text{quand}n \rightarrow \infty\]
	
	\noindent En réécrivant l'équation séquencielle avec ses coefficients asymptotiques $n \rightarrow \infty$
	\[c_{s}\lambda^{s} n^{d_{s}+s\kappa}+c_{s-1}u_{n}\lambda^{s-1} n^{d_{s-1}+(s-1)\kappa}+...+c_{s'}\lambda^{s'} n^{d_{s'}+s'\kappa}u_{n}\]
	
	pour que cette expression s'annule, il est nécessaire que les termes asymptotiquement dominants se compensent, et donc que l'exposant $d_{k}+k\kappa$ le plus grand soit atteint au moins deux fois. Alors $-\kappa$ doit être parmi les pentes du \textit{polygone de newton} de l'équation.
	
	
	\begin{definition} Le polygone de Newton est l'enveloppe convexe supérieure des points $(k, d_{k}) \in \mathbb{R}$, si E=[A,B] désigne une arête du polygone de Newton, on note $\kappa(E)$ l'opposé de sa pente, et on définit l'équation caractéristique associée à E (ou à $\kappa(E))$ par
		\[\chi_{E}(\lambda)=\sum_{(k,d_{k}) \in E} c_{k}\lambda^{k-t}\]
		Où $(t,d_{t})=A$ l'extrémité gauche du segment E.
	\end{definition}
	
	Remarquons que la somme des degrés des différentes équations caractéristiques est égale à l'ordre de l'équation de récurrence 
	
	\begin{theorem} (Perron-Kreuser)
		Pour toute arête E du polygone de Newton de la récurrence (*) notons $\lambda_{E_{1}},\lambda{E_{2}},...$ les racines de $\chi(E)$ comptées avec leur multiplicité.
		
		(a) supposons que pour toute arête E, les modules $|\lambda_{E_{i}}|$ des racines de $\chi(E)$ sont deux à deux distincts. Alors toute solution non ultimement nulle de (*) satisfait
		\[\frac{u_{n+1}}{u_{n}}\sim \lambda_{E_{i}}n^{\kappa(E)} \hspace{2mm} \text{quand } n \rightarrow \infty\]
		Pour une certaine arête E et un certain i.
		
		(b)Si en outre (*) est réversible, elle admet une base de solution 
		\[(u_{n}^{[E_{i}]})_{E_{i}\leq i \leq deg \chi_{E}}\]
		telle que 
		\[\frac{u_{n+1}^{[E_{i}]}}{u_{n}^{[E_{i}]}}\sim \lambda n^{\kappa(E)} \hspace{2mm} \text{quand } n \rightarrow \infty\]
		
		(c)Dans le cas où il existe E et $i \neq j$ tels que $|\lambda_{E_{i}}|=|\lambda_{E_{j}}|$ les analogues des deux assertions précédentes subsistent mais avec la conclusion plus faible
		\[\limsup_{n \rightarrow \infty } \big|\frac{u_{n}^{E_{i}}}{n!^{\kappa(E)}}\big|^{\frac{1}{n}}=|\lambda_{E_{i}}|\]
	\end{theorem}
	
	Certains résultats sont plus précis dans des cas particuliers et Schäfke et Noble donnent alors une discussion plus précise.
	
	\subsection{Esquisse de l'algorithme}
	
	On commence par étudier le polygone de Newton de la récurrence et les équations caractéristiques de la récurrence, pour délimiter à l'aide du théorème de Perron-Kreuser les comportements asymptotiques possibles de $u_{n}$.
	
	\subsection{Pôles et singularités dominantes}
	
	\begin{definition} Si P $\in \mathbb{Q}[z]$ est un polynôme non réduit à un monôme, on note respectivement
		\[\delta(P)=min\{|\zeta| \neq 0: P(\zeta)=0\} \text{ et } \nu_{\delta}=max\{\nu(\zeta,P):|\zeta|=\delta(P)\}\] 
		On appelle \textit{pôles dominants} d'une fraction rationelle et $\delta$-racines  de son dénominateur, et \textit{singularités dominantes} d'un opérateur différentiel à coefficient polynomiaux celles de son coefficient de tête.  
	\end{definition}
	
	\subsection{Croissance générique des solutions}
	
	\begin{definition} Soit $R \in \mathbb{R}[n]\big< S\big>$ unn opérateur réversible non singulier, d'ordre s. Une solution $(u_{n})$ de la récurrence R.u=0 est alors déterminée de façon unique par ses s premières valeurs. Nous dirons qu'une proposition est vrai pour une solution générique si elle est satisfaite pour $(u_{0},u_{1},...,u_{s-1}) \in \mathbb{R}^{s}/V$ où V est un sous-espace strict de $\mathbb{R}^{s}$
		
	\end{definition}
	
	\begin{algorithm}
		\caption{Asympt(R)}
		
		\vspace{2mm}
		
		\textbf{Entrée:} $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{4mm} \in \mathbb{Q}[n]\big<S\big>$
		
		\textbf{Sortie:} $\kappa \in \mathbb{Q}, P_{\alpha} \in \mathbb{Q}[z].$
		
		\begin{algorithmic}[1]
			\vspace{4mm}
			\STATE $\kappa:= max_{k=0}^{s-1}\frac{\text{deg } b^{[k]}-\text{deg } b{[s]}}{s-k}$
			\vspace{4mm}
			\STATE $P_{\alpha}:=\sum_{l=0}^{s} b_{d+l\kappa}^{[s-l]}\text{ où } d=\text{deg } b^{[s]}$
			\vspace{4mm}
			\STATE Renvoyer $(\kappa,P_{\alpha})$
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent D'après le théorème de Perron-kreuser les solutions dont "la croissance est la plus rapide", c'est à dire celle dont le coeffficient $\kappa$ est le plus grand est celle la plus à droite du polygone de Newton, et dont la racine est celle de module maximal dans son équation caractéristique.
	
	l'algorithme renvoie le $kappa$ maximal et le polynôme réciproque de l'équation caractéristique correspondante au coefficient $\kappa$.
	
	\begin{proposition} Posons $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{4mm} \in \mathbb{Q}[n]\big<S\big>$ et supposons que R n'est pas réduit à un terme $b^{[s]}S^{k}$. On a donc
		\[\limsup_{n \rightarrow \infty }\Big|\frac{u_{n}}{n!^{k}}\Big|^{\frac{1}{n}} \hspace{4mm} \text{où } \alpha=\frac{1}{\delta(P_{\alpha})}\]
		Pour toute solution $(u_{n})$ vérifiant R.u=0 avec égalité pour une solution générique. 
	\end{proposition}
	\textbf{Esquisse de démonstration}
	Le polygone de Newton étant convexe, le coefficient $\kappa$ le plus élevé correspond à un segment relié au sommet le plus à droite du polygone.\\
	Aussi, en écrivant prenant le polynôme caractéristique correspondant, le dérivant par $\lambda^{s}$ et en posant $\beta=\frac{1}{\lambda}$ le polynôme en $\beta$ est le même que celui renvoyé par l'algorithme \textit{asympt}. Pour ce qui est du rayon de l'inégalité, elle résulte du (c) du théorème de Perron-Kreuser.
	Il reste à démontrer l'inégalité pour des conditions initiales génériques. Soit $V=ker$ $R \subset \mathbb{Q}^{\mathbb{N}}$, d'après le théorème (vrai?????)\\
	Il existe une solution $u^{[0]}$ telle que $\limsup \big|u^{[0]}_{n}/n!^{k}\big|^{1/n}=\alpha$.\\
	Étendons cette solution en une base $(u^{[0]},u^{[1]},..,u^{[s-1]})$ de V. Soit $u=\sum_{k}\lambda^{[k]}u^{[k]}$
	Par construction de $\kappa$ et $\alpha$, on a $\limsup \big|u_{n}/n!^{\kappa}\big|^{1/n} \leq \alpha$. Quitte à extraire des sous-suites pour $u_{n}$ on peut supposer que $u^{[0]}_{n}$ n'est jamais nul,il existe donc $\beta$ tel que
	\[\Big|\lambda^{[0]}+\frac{\lambda^{[1]}u^{[1]}_{n}+...+\lambda^{[s-1]}u^{[s-1]}_{n}}{u^{[0]}_{n}}\Big| \rightarrow_{n \rightarrow \infty} \frac{\beta}{\alpha}\] 
	et $\beta=\alpha$ à moins que 
	\[\frac{\lambda^{[1]}u^{[1]}_{n}+...+\lambda^{[s-1]}u^{[s-1]}_{n}}{u^{[0]}_{n}}\rightarrow_{n \rightarrow \infty}-\lambda^{[0]}\]
	Condition fausse pour des $\lambda^{[k]}$ génériques.
	
	\begin{definition} On appelle arête dominante l'arête la plus à droite du polygone de Newton, équation caractéristique dominante son équation caractéristique associée, et récurence normalisée une récurrence pour laquelle cette arête est horizontale. C'est à dire que le comportement asymptotique est purement exponentiel, et non factoriel ($\kappa=0$).
	\end{definition}
	
	\subsection{Fonction génératrice normalisée}
	
	\begin{algorithm}
		\caption{RecToDiffeq}
		
		\vspace{2mm}
		
		\textbf{Entrée:} $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{4mm} \in \mathbb{Q}[n]\big<S\big>$
		
		\textbf{Sortie:} Un opérateur $D \in \mathbb{Q}[z] \big<\theta\big>$ tel que $\forall (u_{n}) \in \mathbb{Q}^{\mathbb{N}}$ R.u=0 $\iff D.\sum u_{n}x^{n}=0$ 
		
		\begin{algorithmic}[1]
			\vspace{3mm}
			\STATE g:=pgcd$(b^{[s]},\pi)$ où $\pi=\prod_{k=1}^{s}(n+k)$
			\vspace{3mm}
			\STATE calculer les $c_{jk}$ tels que $g.R=\sum_{k=0}^{s}\sum_{j}c_{jk}n^{j}S^{k}$\\
			\vspace{3mm}
			$[\textit{on a ainsi } g.R=\sum_{k=0}^{s}\sum_{j}c_{jk}S^{k}(n-k)^{j}]$
			\vspace{3mm}
			\STATE développer $\sum_{k=0}^{s}\sum_{j} c_{jk}z^{s-k}(\theta-k)^{j}$ sous la forme $D=\sum_{k=0}^{r} a^{[k]}\theta^{k}$
			\vspace{3mm}
			\STATE renvoyer D
		\end{algorithmic}
		
	\end{algorithm}
	La multiplication par g s'assure que les premier termes $u_{0},...,u_{s-1}$
	s'affranchissent des potentielles contraintes crées par les termes indéxés sur les coefficients négatifs. Mais ne change pas la relation pour les termes supérieurs à s.
	
	Considérons $R \in \mathbb{Q}[n]\big< S\big>$ un opérateur non singulier, d'ordre s. Une solution $(u_{n})$ de la récurrence R.u=0, u(z) est annulée par l'opérateur RecToDiffeq$(R)(\theta)=\sum_{k=0}^{r}a^{[k]}\theta^{k} \in \mathbb{Q}[z]\big< \theta\big>$. En divisant par $a^{[r]}$ on obtient
	\begin{equation}
	\big(\theta^{r}+\frac{a^{[r-1]}\theta^{r-1}+...+a^{[0]}}{a^{[r]}}\big).u=0
	\end{equation}

	
	\begin{proposition}Si l'opérateur R est normalisé, alors l'origine est un point régulier de D=RecToDiffeq et l'équation caractéristique dominante de $R$ est le polynôme réciproque de $a^{[r]}$ 
	\end{proposition}
	
	Dans le cas général, on commence par normaliser R.
	Le produit symétrique n'est pas quelconque, regarder Barkatou et al.
	\begin{algorithm}
		\caption{Normalize$(R,\kappa)$}
		
		\vspace{2mm}
		
		\textbf{Entrée:} Un opérateur de récurrence $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \in \mathbb{Q}[n]\big<S\big>$, un rationnel $\kappa$
		
		\textbf{Sortie:} Un opérateur $D \in \mathbb{Q}[x] \big<\theta\big>$ 
		
		\begin{algorithmic}[1]
			\vspace{4mm}
			\STATE $p/q:=\kappa$, avec $p \in \mathbb{Z}$, $q \in \mathbb{N}^{*}$, pgcd$(p,q)=1$
			\vspace{4mm}
			\STATE calculer les coefficients $\hat{b}^{[k]}(n)$ du produit symétrique $\hat{R}=\sum_{k=0}^{qs}\hat{b}^{[k]}(n)S^{k}$\\
			\vspace{2mm}
			de $R$ par $(n+k)^{p}S^{q}-1$
			\vspace{4mm}
			\STATE renvoyer RecToDiffeq($\hat{R}$)
		\end{algorithmic}
		
	\end{algorithm}
	
	
	\begin{proposition}Soit $R \in \mathbb{Q}[n]\big< S\big>$ un opérateur non singulier, réversible de coefficient constant par rapport à S non nul. Soient $p/q, P_{\alpha}$ le comportement asymptotique génériquee renvoyé par $Asympt$. On suppose que $\delta(P_{\alpha})<\infty$ alors $Normalize(R,p/q)$ calcule un opérateur différentiel D qui annule la série 
		\[u(x)=\sum_{n=0}^{\infty} \psi_{n}u_{n}x^{n}\]
		pour toutes suites $\psi$ et $u$ solutions de
		\[(n+q)^{p}\psi_{n+q}=\psi_{n}\]
		et $R.u=0$, l'opérateur D est régulier à l'origine et le module de sa singularité dominante est égale à $\delta(P_{\alpha})$.
	\end{proposition}
	
	\subsection{Séries majorantes}
	
	\begin{definition} Une série formelle v $\in \mathbb{R}_{+}[[\mathbb{Z}]]$ est appelée série majorante de $ u \in \mathbb{R}[[\mathbb{Z}]$ si v domine u coefficient par coefficient, $\forall n$ $ |u_{n}|\leq v_{n}$. On note $u \unlhd v$.
	\end{definition}
	
	\begin{proposition} Soient $u,u^{[1]},u^{[2]}\in \mathbb{R}[[x]]$, et $v,v^{[1]},v^{[2]} \in \mathbb{R}_{+}[[x]]$ tels que $u\leq v,u^{[1]}\leq v^{[1]}$ et $u^{[2]}\leq v^{[2]}$ alors
		
		(a)Le rayon de convergence de $v$ est inclus dans celui de $u$;
		
		(b)Si $\zeta$ appartient au disque de convergence de $v$, alors $|u(\zeta)|\leq v(|\zeta|)$;
		
		(c)On a les majorations
		\[u' \unlhd v'; \hspace{6mm} u^{[1]}+u^{[2]}\unlhd v^{[1]}+v^{[2]}; \hspace{6mm}u^{[1]}u^{[2]}\unlhd v^{[1]}v^{[2]}; \]
		
		(e)Si $v^{[1]}(0)=0$ alors $ u^{[2]} \circ u^{[1]} \unlhd v^{[2]}\circ v^{[1]}$.
		
	\end{proposition}
	
	\noindent\textbf{Méthode de majoration avec un example simple}\\
	On choisit une série a(z) définie sur $D=\{z,|z|<\rho\}$ soit $u$ une autre série vérifiant $u'(z)=a(z)u(z)$, alors le rayon de convergence de $u$ est le même que celui de a.\\
	La fonction série un rayon de convergence $\rho$ donne à s'intéresser à une majoration de la forme
	\[M(z)=\frac{m}{(1-\rho^{-1} z)^{\kappa}}\] 
	
	\noindent\textbf{Méthode} Pour commencer, $\exists M \in \mathbb{R}_{+}, \forall n$ $|a_{n}|\leq M \rho^{-n}$.
	On écrit la relation sur u en termes de séries: $(n+1)u_{n+1}=\sum_{k=0}^{n}u_{n-j}a_{j}$ en considérant $v$ telle que  $(n+1)v_{n+1}=\sum_{k=0}^{n}v_{n-j}M\rho^{-j}$ on observe $v'(z)=\frac{M}{1-z\rho^{-1}}v(z)$ \\
	On a $v=v_{0}(1-z\rho^{-1})^{-M\rho}$. Si $|u_{0}| \leq v_{0}$ la récurrence permet de conclure $ u\unlhd v$. On sait que le rayon de convergence de $v$ est $\rho$ donc celui de $u$ est supérieur à $\rho$, la condition $u'(z)=a(z)u(z)$ permet de conclure l'égalité.
	
	\subsection{Séries majorantes pour les fonctions D-finies}
	Dans l'example que nous avons vu précèdement, les séries majorantes servent à majorer des fonctions, elle sont utilisées sur les restes des séries pour connaître la précision de l'approximation de la fonction étudiée.
	Le cas qui pourrait paraître singulier de l'équation différentielle normalisée est en fait le plus courant puisqu'il correspond au cas où l'on est à distance finie non nulle de la singularité la plus proche.
	
	
	\[P(z,\theta) \cdot u(z)=\big[\theta^{r}p_r(z)+\theta^{r-1}p_{r-1}(z)+...+p_0(z)\big]\cdot u(z)=0\]
	Les polynômes $p_0,p_1,...,p_r$ peuvent être considérés premiers entre eux sans perte de généralité. Soit
	 \[\overset{\sim}{u}(z)=\sum_{k=0}^{N}u_n z^n\hspace{2mm}\text{avec}\hspace{2mm}u(z)-\overset{\sim}{u}(z)=R_u^{N+1}(z)\] 
	alors 
	\[  P(z,\theta) \cdot u(z)-\overset{\sim}{u}(z)=P(z,\theta)\overset{\sim}{u}(z)=q(z)\]
	Où $q(z)$ est de la forme $q_Nz^N+...+q_N+sz^{N+s}$ où $s= \deg_{z}P(z,\theta) $ 
	\\
	On a alors l'équation majorante suivante
	\[y(z)=p_r(z)(\overset{\sim}{u}(z)-u(z))\]
	Supposant que $p_r(0) \neq 0$ c'est à dire qu'on soit sur un point régulier,
	\[L(z,\theta)\cdot y(z)=\Big[\theta^{r}+\frac{\theta^{r}p_{r-1}(z)+...+p_0(z)}{p_r(z)}\Big]\]
	On réécrit $L(z,\theta)$ en développant $p_r^{-1}$ en série entière
	\[L(z,\theta)=\sum_{j\geq 0} Q_{j}(\theta)z^j.\]
	Le polynôme $Q_0$ est de degré $r$ alors que pour $j \geq 1$ le degré des polynômes est inférieur à r-1.
	En repassant l'équation sous forme séquentielle, on a 
	\[L(S^{-1},n)\cdot (y_n)_{n \in \mathbb{Z}}=\sum Q_j(n)S^{-j} \cdot  (y_n)_{n \in \mathbb{Z}} =(q_n)_{n \in \mathbb{Z}} \]
	Alors
	\[y_n=\frac{q_n-\sum_{j\geq1}Q_j(n)y_{n-j}}{Q_0(n)}= \frac{1}{n}\Big[\frac{nq_n}{Q_0(n)}-\sum_{j\geq1}\frac{nQ_j(n)y_{n-j}}{Q_0(n)}\Big]\tag{*}\]
	Avec ce qui a été dit précèdement, les coefficients $nq_n/Q_0(n)$ et $nQ_j(n)/Q_0(n)$ sont bornés.
	Supposons qu'on ait $\hat{q}_n$ et $\hat{a}_n$ bornées aussi telles que
	\begin{equation}
	|nq_n/Q_0(n)|\leq \hat{q}_n,  \hspace{4mm}\forall n\geq n_0\\
	\end{equation}
	\begin{equation}
	|nQ_j(n)/Q_0(n)|\leq \hat{a}_n,\hspace{3mm}  \forall n\geq n_0, \hspace{2mm} j\geq 1
	\end{equation}
	avec $\hat{a}_j=O(\alpha^j)$ pour un certain $\alpha$ quand $j \rightarrow \infty$. On peut alors déduire de (*).
	\[ \leq|\hat{y}_n|=\frac{1}{n}\Big(\hat{q}_n+\sum_{j\geq1}\hat{a}_{n}\hat{y}_{n-j}\Big)\tag{1}\]
	Dans ce cas $\hat{y}_n$ est une série majorante de $y_n$. En traduisant l'équation (1) en équation différentielle on obtient
	\[[\theta-\hat{a}(z)] \hat{y}(z) = \hat{q}(z)\]
	On a 
	
	
	
	\section{Étude de la fonction de majoration et détermination d'un majorant de alpha}
	La fonction de majoration renvoyée par la fonction Bound NormalDiffeq$(D,P_{\alpha},u)$  est de la  forme
	\[g(z)=\frac{A}{(1-\alpha z)^{K}} \hspace{2mm}\text{si T=0}\hspace{6mm} g(z)=\text{A exp}(\frac{K/T}{(1-\alpha z)^{T}}) \hspace{2mm} \text{sinon.} \]
	
	\noindent L'objectif ici est de borner le coefficient $\gamma(f,z)=\sup_{k \geq 2} \left\|f'(z)^{-1}\frac{f^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}$ en utilisant la fonction majorante développée dans la partie précèdente. Elle respecte la propriété interessante $f\unlhd g$ et donc trouver $\alpha(g,z)$ sur cette fonction reviendrai à majorer $\alpha(f,z)$. L'objectif serait de trouver N à partir duquel $\left\|g'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}$ est décroissante. Alors 
	\[\gamma(g,z)=max_{k=2}^{N}\left\|g'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}\] 
	
	\begin{proposition} Dans le cas $T=0$ on a \[\gamma(g,z)=max_{k=2}^{\lceil e^{1}K!^{\frac{1}{K-1}} \rceil}\left\|g'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}\]
	\end{proposition}

	\noindent\textbf{Démonstration}
	\noindent En étudiant l'expression de la fonction de majoration pour $T=0$ on obtient
	\[g^{(k)}(z)=\frac{A\alpha^{k}\prod_{i=0}^{k-1}(K+i)}{(1-\alpha z)^{K+k}}\]
	On peut écrire,
	\[\frac{g^{(k)}(z)}{k!}=\frac{A\alpha^{k}}{(1-\alpha z)^{K+k}} \frac{\prod_{i=1}^{K-1}(k+i)}{(K-1)!} \]
	On a finalement, pour $z < \alpha$ 
	\[\left\|g'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}= \frac{\alpha}{1-\alpha z} \left\|\frac{\prod_{i=1}^{K-1}(k+i)}{K!}\right\|^{\frac{1}{k-1}} \tag{*}\]
	Considérons cette expression comme une fonction de $\mathbb{R}$ dans $\mathbb{R}$. On a donc un polynôme de degré $K-1$.
	\[P(k)=\frac{\prod_{i=1}^{K-1}(k+i)}{K!}\]
	
	
	\noindent Étudions la fonction $h(x)=(f(x))^{\frac{1}{x-1}}$ où f est strictement positive, on peut la réecrire
	\[h(x)=\text{exp}(\frac{ln(f(x))}{x-1}) \]
	Une étude de signe donne à nous intéresser au signe de l'expression
	\[(x-1)f'(x)-f(x)ln(f(x)) \tag{1}\] 
	qui détermine le signe de la dérivée de h. On regarde donc l'expression 
	\[(k-1)P'(k)-P(k)log(P(k))=P(k)\Big[\big(\sum_{i=1}^{K-1}\frac{k-1}{k+i}\big)-log(P(k))\Big]\]
	On majore $\frac{k-1}{k+i}$ par 1 et minore $P(k)$ par $\frac{k^{K-1}}{K!}$ On est sûr que la série est décroissante lorsque 
	\[K-1-ln\big(\frac{k^{K-1}}{K!}\big)<0 \Longleftrightarrow e^{1}K!^{\frac{1}{K-1}}< k\]
	On a finalement 
	\[\gamma(f,z)=max_{k=2}^{\lceil e^{1}K!^{\frac{1}{K-1}} \rceil}\left\|f'(z)^{-1}\frac{f^{(k)}(z)}{k!}\right\| \]
	
	\vspace{1cm} 
	
	On s'intéresse maintenant au cas où $T\neq 0$ de l'expression, on veut développer $f(z)$ dans le cas où $T \neq 0$. On ne s'en est pas préoccupé précèdement, mais ici, la constante A ne servira pas dans la majoration, la constante sera absorbée dans division par $f'(z)$. En posant \[\alpha_{k}(z)=\frac{\big(\frac{1}{(1-\alpha z)^{T}}\big)}{k!}^{(k)}=\frac{\alpha^{k}}{(1-\alpha z)^{T+k}} \frac{\prod_{i=1}^{T-1}(k+i)}{(K-1)!},\] On a,
	\[g(z+t)=\text{A exp}(\frac{K/T}{(1-\alpha (z+t))^{T}})\]
	En intégrant ce qui a été dit précèdement et en développant en série entière à l'intérieur de l'exponentiel on s'intéressera à
	\[f(z+t)=\text{exp}\Big(K/T \sum_{k=1}^{+\infty}\alpha_{k}(z)t^{k} \Big)\]
	
	\[= 1+(K/T)\frac{\sum_{k=1}^{+\infty}\alpha_{k}(z)t^{k}}{1!}+(K/T)^{2}\frac{\big(\sum_{k=1}^{+\infty}\alpha_{k}(z)t^{k}\big)^{2}}{2!}+...+(K/T)^{l}\frac{\big(\sum_{k=1}^{+\infty}\alpha_{k}(z)t^{k}\big)^{l}}{l!}+...\]
	En identifiant les coefficients correspondants à $t^{k}$
	\[\frac{f^{(k)}(z)}{k!}=\sum_{i=1}^{k}\frac{(K/T)^{i}}{i!}\Big(\sum_{r_{1}\leq r_{2}\leq ...\leq{r_{i}}}\Pi(r_{1},...,r_{i})\prod_{h=1}^{i}\alpha_{r_{h}}(z) \Big) \hspace{2mm} \text{avec }\sum_{h=1}^{i} r_{h}=k\]
	Où $\Pi(r_{1},...,r_{i})$ est définie comme suit:\\
	On regroupe les $r_{h}$ en $g$ groupes $\Gamma$ de coefficients égaux, soit $\Gamma_{h}$ leur taille telle que $\sum_{h=1}^{g}\Gamma_{h}=i$, alors
	\[\Pi(r_{1},r_{2},...,r_{i})=\dbinom{i}{\Gamma_{g}}\times \dbinom{i-\Gamma_{g}}{\Gamma_{g-1}}\times ...\times \dbinom{\Gamma_{1}}{\Gamma_{1}}=\frac{i!}{\Gamma_h! \times \Gamma_{h-1}! \times ... \times \Gamma_1!}\]
	
	
	\newpage
	\section{Les méthodes d'exclusion de zéros}
	\subsection{L'algorithme de Bissection-exclusion de J.-C. Yakoubsohn}
	L'idée fondamentale de l'algorithme de Jakoubsohn est de constuire une fonction $M(f,x)$ strictement décroissante sur $\mathbb{R}_+$ telle que si cette fonction ne s'annule pas sur un intervalle $[x-\alpha,x+\alpha]$, on peut assurer que la fonction f ne s'annule pas.
	
	\begin{definition}Soit f une fonction analytique en x, alors pour t suffisamment petit\\ $f(x+t)=f(x)+\sum_{k=1}^{+\infty} \frac{f^{(k)}(x)}{k!}t^{k}$. On définit $M(f,x)(t)=|f(x)|-\sum_{k=1}^{+\infty}|\frac{f^{(k)}(x)}{k!}|t^{k}$
	\end{definition}
	
	\begin{proposition} Si $M(f,x)(\tau)=0$ alors la fonction $f$ ne s'annule pas sur $]x-\tau,x+\tau[$ 
	\end{proposition}

	\begin{algorithm}
		\caption{bisection-exclusion method}
		
		\vspace{2mm}
		
		\textbf{Entrée:} fonction D-finie $f(x)=\sum_{n \geq 0} \mathbb{f}(n)x^{n}$. interval (a,b)
		
		\textbf{Sortie:} $(a_{1},b_{1})\cup (a_{2},b_{2})$
		
		\begin{algorithmic}[1]
			
			\STATE $g(t)=:M(f,\frac{a+b}{2})(t)$
			\STATE c=premier\_zero\_positif(g)
			\STATE Renvoyer($(a,\frac{a+b}{2}-c)\cup (\frac{a+b}{2}+c,b)$)
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent Ici, on n'a qu'une étape de l'algorithme et l'idée sera bien sûr d'appliquer récursivement cette fonction pour avoir des intervalles de plus en plus précis. L'article de Yakoubsohn donne une discussion plus complète sur la taille des intervalles en fonctions des "clusters" de zéros.\\
	\noindent maj: fonction de majoration fournie dans le module Ore\_algebra.analytic de Marc Mezzarobba\\
	zéropositif(g): le premier zéro sur lequel la fonction g s'annule sur ${R}$
	\\
	

	
	\subsection{Séparation des zéros via la méthode de Rouché}
	Nous allons ici nous intéresser au moyen de certifier nos zéros. On sait que les zéros des fonctions analytiques sont isolés, mais on cherche ici évaluer concrètement la distance entre deux zéros. Cela a été fait dans le livre de Dedieu et ici nous chercherons à adapter les démonstrations appliquées à gamma, aux séries majorantes.
	
	\begin{definition} Soient $\mathbb{E}$ et $\mathbb{F}$ des espaces de Banach et 
		Soit $f:\mathbb{E}\rightarrow \mathbb{F}$ une fonction analytique, notons $\zeta$ un point de $\mathbb{E}$ tel que $f(\zeta)=0$, alors on note
		\begin{equation}
		 sep(f,\zeta)=\inf_{f(\zeta '=0, \hspace{1mm} \zeta \neq \zeta ')}\left\|\zeta-\zeta '\right\|
		\end{equation}
		
	\end{definition}
	
	Le théorème suivant est le théorème de Rouché, il est général, on l'utilisera pour le cas particulier n=1.

	\begin{theorem}(Rouché) Donnons nous un domaine borné $D \subset \mathbb{C}^{n}$ de frontière de Jordan S et deux application analytiques f et g définies sur un voisinage ouvert de D et à valeurs dans $\mathbb{C}^n$. Si pour tout $z \in S$ 
		\[\left\|f(z)\right\|>\left\|g(z)\right\|\]
	alors $f+g$ admet autant de zéros (comptés avec multiplicités) que f dans D.
	\end{theorem}
	
	
	\section{De la théorie alpha de Smale aux séries majorantes}
	
	La suite sera une adaptation de la théorie alpha de Smale aux séries majorantes, l'objectif de cettte partie est de fournir des résultats plus généraux que la théorie de gamma pour n'importe quelle série majorante. L'inconvénient c'est que les conditions "légères" de la théorie alpha seront remplacés par des conditions plus "lourdes". \\
	Nous aurons besoin d'introduire des notations pour parler de la série majorante sur le reste d'une série à l'ordre k.
	
	\begin{definition} Soit f une fonction analytique en x, alors on peut écrire $f(x+t)=\sum_{k \geq 0} \frac{f^{(k)}(x)}{k!}t^{k}$, Le reste de la série à l'ordre k est 
	\[R_{f}^{k}(x,t)=\sum_{i \geq k} \frac{f^{(i)}(x)}{i!}t^{i}\]
	et la série majorante de ce reste
	\[M_{f}^{i}(x,t) \unrhd R_{f}^{i}(x,t)\]
	\end{definition}
	
	\subsection{Zéros répulsifs}
	On définit trois ensembles pour notre algorithme.
	\begin{definition} Soient les trois ensembles suivants.
		
	$\mathbb{A}_i=\{[c,d], \exists ! \zeta \text{ de multiplicité i } \in [c,d], f(\zeta)=0\}$
	
	$\mathbb{E}=\{[c,d], \not\exists \zeta \in [c,d], f(\zeta)=0\}$
	
	$\mathbb{I}=\{[c,d] \text{ indéterminés }\}$\\
	\end{definition}

	\noindent On cherche à rajouter des intervalles dans l'ensemble $\mathbb{A}_1$, c'est à dire des intervalles contenant un unique zéro de multiplicité 1.
	Avant d'énoncer le théorème, on définit la fonction $S$ dont nous aurons besoin par la suite. 
	\begin{definition} Soit $f$ une fonction définie de $\mathbb{R}$ dans $\mathbb{R}_+$, croissante sur $\mathbb{R}_+$ on définit la fonction $S:(\mathbb{R}_+ \rightarrow \mathbb{R}) \longrightarrow \mathbb{R}_+$  telle que $S(f)$ renvoie le plus petit $x \in \mathbb{R}_+$ tel que $f(x)=1$, elle renvoie 0 si $f(0) \geq1$. On  peut l'écrire formellement
		\begin{equation}
		S(f)=
		\left\lbrace
		\begin{array}{ccc}
		0  & \mbox{si} & f(0) \geq 1\\
		\inf \{x \in f^{-1}(1)\cap \mathbb{R}_{+}\} & \mbox{sinon}\\
		\end{array}\right.
		\end{equation}
	\end{definition}
	\vspace{7mm}
	Pour alléger les notations dans la suite on pose $\mathbb{S}_{f}(\zeta)=S\Big(|f'(\zeta)^{-1}|\frac{M_{f}^{2}(\zeta)(t)}{t}\Big)$\\
	Rappel $\beta(f,\zeta)=|f'(\zeta)^{-1}f(\zeta)|$, c'est aussi la longueur de pas de l'itération de Newton.
	\vspace{7mm}
	\begin{theorem}	Soit f une fonction, $\zeta$ et $\zeta '$ deux points de $\mathbb{R}$ tels que $f'(\zeta)\neq 0$ et que $f(\zeta)=f(\zeta ')=0$ et $|\zeta '-\zeta|$ inférieur au rayon de convergence du développement en série entière de $f$ en $\zeta$. 
		
		\[Sep(f,\zeta) \geq S\Big(|f'(\zeta)^{-1}|\frac{M_{f}^{2}(\zeta)(t)}{t}\Big) \hspace{4mm} \]
	\end{theorem}

	\begin{demonstration}Raisonnons par l'absurde, 
	\[f(\zeta ')=f(\zeta)+(\zeta '-\zeta)f'(\zeta)+ \sum_{k \geq 2}\frac{f^{(k)}(\zeta)}{k!}(\zeta'-\zeta)^k\]
	qui se réecrit, en multipliant par $f'(\zeta ')^{-1}$
	\[(\zeta-\zeta')=f'(\zeta)^{-1} \sum_{k \geq 2}\frac{f^{(k)}(\zeta)}{k!}(\zeta'-\zeta)^k]\]
	d'où 
	\[1 \leq |f'(\zeta)^{-1}|\sum_{k \geq 2}  \big|\frac{f^{(k)}(\zeta)}{k!}(\zeta'-\zeta)^{k-1} \big|\leq |f'(\zeta)^{-1}| \frac{M_{f}^{2}(\zeta)(|\zeta-\zeta'|)}{|\zeta-\zeta'|}\]
	\[1 \]
	Or $|\zeta-\zeta'|<S\Big(|f'(\zeta)^{-1}|\frac{M_{f}^{2}(\zeta)(t)}{t}\Big)=\mathbb{S}_{f}(\zeta)$ la fonction majorante est croisssante alors $1<1$, c'est absurde.
	
	\noindent Il s'en suit de ce théorème qu'il n'y a pas de zéro autre que $\zeta$  dans l'intervalle $[\zeta-\mathbb{S}_{f}(\zeta),\zeta+\mathbb{S}_{f}(\zeta)]$
	\end{demonstration} 
	\noindent Remarque, si la fonction majorante $M_{f}^{2}(\zeta)$ est $o(t)$ en zéro alors $\mathbb{S}_{f}(\zeta)>0$. Ça sera le cas des séries majorantes que nous utilisierons.
	\subsection{Quantification de la distance au zéro}
	
	\begin{theorem} Soit $f:\mathbb{C} \rightarrow \mathbb{C}$ une fonction analytique, soit $x_0 \in \mathbb{C}$ tel que $f'(x_0) \neq 0$. Si 
	\[\beta(f,x_0)<\sup_{r \in [0,\mathbb{S}_{f}(x_0)]} r-|f'(\zeta)^{-1}|\frac{M_{f}^{2}(\zeta)(r)}{r}\]
	 alors on peut assurer que quel que soit $r\in [0,\mathbb{S}_{f}(x_0)]$ vérifiant
	 \[r-|f'(\zeta)^{-1}|\frac{M_{f}^{2}(\zeta)(t)}{r}>\beta(f,x_0)\]
	 existe un unique zéro pour f sur $B(x_0,r)$ la boule ouverte centré en $x_0$ de rayon $r$. 
	\end{theorem}
	 Nous montrerons que dans le cas réel que $]x_0-\tau,x_0+\tau[$ contient un unique zéro. Ce qui vient du fait qu'une racine complexe admet aussi son conjugué comme racine, hors la boule trouvé est centrée sur un point de l'axe des réels donc il devrait y avoir 2 zéros et non pas 1 dans la boule.
	 
	 \vspace{7mm}	
	 
	
	\begin{demonstration}
	
	Définissons $g(x)=f(x)-f(x_0)$, $g(x_0)=0$, $Dg(x_0)=f'(x_0)$ d'après le théorème précèdent, $x_0$ est le seul zéro de g dans la boule
	$B(x_0,\mathbb{S}_{f}(x_0))$
	La série de Taylor de g est convergente, elle est donnée par
	\[g(x)=f'(x_0)(x-x_0)+ \sum_{k \geq 2}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\]
	Nous allons prouver que pour $0<r<\mathbb{S}_{f}(x_0)$ on a,
	\[|f'(x_0)^{-1}f(x)-f'(x_0)^{-1}g(x)|=|f'(x_0)^{-1}f(x_0)|<|f'(x_0)^{-1}g(x)|\]
	\\
	Pour tout $x$ avec $|x-x_0|=r$
	\\
	Par le théorème de Rouché $|f'(x_0)^{-1}|f$ et $|f'(x_0)^{-1}|g$ auront le même nombre de zéros dans cette boule. Remarquons que 
	\[|f'(x_0)^{-1}f(x)-f'(x_0)^{-1}g(x)|=|f'(x_0)^{-1}f(x_0)|=\beta(f,x_0)\]
	Aussi
	\[x-x_0=f'(x_0)^{-1}g(x)-\sum_{k \geq 2}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\]
	de sorte que
	\[r \leq |f'(x_0)^{-1}g(x)|+M_{f}^{2}(x_0)(r) \]
	C'est à dire que l'hypothèse de Rouché est satisfaite si
	\[\beta(f,x_0)<r-M_{f}^{2}(x_0)(r)\] 
	Ce sera cette expression qui permettra d'exclure des intervalles. Pour commencer on peut dire que si 
	\[\beta(f,x_0)< \sup_{r \in [0,\mathbb{S}_{f}(x_0)]}\{r-M_{f}^{2}(x_0)(r)\}\]
	alors, pour tout r tel que l'inégalité  $\beta(f,x_0)<r-M_{f}^{2}(x_0)(r)$ soit satisfaite, on peut assurer avec le théorème de Rouché qu'il n'existe qu'un seul zéro dans l'intervalle $[x_0-r,x_0+r]$
	
	\end{demonstration} 
	 
	
	
	\section{Taille minimale des intervalles ajoutés à $\mathbb{A}_1$}
	On peut maximiser la taille de l'intervalle dans 
	\[\tau=\sup \Big\{t \in [0,\mathbb{S}_{f}(x_0)],  t-|f'(\zeta)^{-1}|\frac{M_{f}^{2}(\zeta)(t)}{t}>\beta(f,x_0)\Big\} \]
	alors, il existe au moins un $t$ supérieur à $r_0$ le point où le sup est atteint dans $[0,\mathbb{S}_{f}(x_0)]$ tel que $t-M_{f}^{2}(x_0)(t)>\beta(f,x_0)$.
	Mais pour tout $t$ tels que cette inégalité est satisfaite on a
	\[|f'(x_0)^{-1}g(x)|\geq t-M_{f}^{2}(x_0)(t)>\beta(f,x_0)=|f'(x_0)^{-1}f(x)-f'(x_0)^{-1}g(x)| \tag{*}\]
	On peut appliquer le théorème de Rouché sur la boule $B(x_0,t)$, mais on a alors
	\[B(x_0,\tau)=\bigcup_{t}B(x_0,t) \hspace{4mm} \]
	t vérifiant (*) $\tau$ celui défini dans l'énoncé ne contient lui même qu'un seul zéro.
\end{document}


