
\documentclass[a4paper,10.5pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[utf8]{inputenc}
\textwidth15cm
\oddsidemargin+0.25cm
\textheight24cm
\topmargin-2cm


\title{Zéros certifiés des fonctions D-finies}
\author{Mathis Deronzier}
\date{}

\begin{document}
	
	\maketitle
	\renewcommand{\contentsname}{Sommaire}
	\newpage
	\tableofcontents
	\newpage
	\renewcommand {\algorithmicrequire } {\textbf{\textsc{Entrée(s):} } }
	\renewcommand {\algorithmicensure } {\textbf{\textsc{Sortie:} } }
	\renewcommand {\algorithmicwhile } {\textbf{Tant que} }
	\renewcommand {\algorithmicdo } {\textbf{faire} }
	\renewcommand {\algorithmicendwhile } {\textbf{fin du Tant que} }
	\renewcommand {\algorithmicif } {\textbf{Si} }
	\renewcommand {\algorithmicthen } {\textbf{alors} }
	\renewcommand {\algorithmicendif } {\textbf{fin du Si} }
	\renewcommand {\algorithmicelse } {\textbf{Sinon} }
	\renewcommand {\algorithmicreturn } {\textbf{Renvoyer} }

	\section{Introduction}
	\newtheorem{theorem}{Théorème}[section] 
	\newtheorem{proposition}{Proposition}
	\newtheorem{corollaire}{Corollaire}
	\newtheorem{definition}{Définition}
	\newtheorem{demonstration}{Démonstration}
	\subsection{Remerciments}
	\subsection{Cadre}
	
	Le problème que nous cherchons à résoudre est le suivant, nous avons une fonction D-finie donnée par son équation différentielle et ses conditions initiales et nous allons chercher à trouver tous les zéros de cette fonction sur un segment $[a,b]$ tel que la fonction ne contient aucune singularité sur ce segment. \\
	Premièrement, les fonctions D-finies ont été définies sur le corps $\mathbb{C}$ initialement, cependant notre problème se concentre sur les zéros sur $\mathbb{R}$, la fonction est donc $f: \mathbb{R} \rightarrow \mathbb{R}$.
	Ensuite, le nombre de zéros d'une fonction analytique est borné sur un compact, alors on cherchera bien un nombre fini de zéros.
	Et finalement qu'est-ce que veut dire que trouver un zéro sur un ordinateur? La nature même de la représentation des nombres sur ordinateur ne permet de manipuler que des nombres rationnels, alors il est évident que l'on ne pourra pas trouver tous les zéros exactes des fonctions. On ne manipulera pas ici d'expression symbolique sur ordinateur. Alors, il faudra signifier ce que veut dire "trouver un zéro".
	
	\subsubsection{Plan}
	
	Tout d'abord il faudra nous familiariser avec la notion de fonction D-finie, ça sera l'objectif du premier chapitre, on introduira les propriétés des fonctions D-finies que nous utiliserons dans la suite du travail.
	
	La troisième partie est une partie "inutile", puisque, même si elle a un intérêt théorique, nous expliquerons pourquoi il est difficile d'utiliser cette méthode certifier tous les zéros d'un segment.
	
	On introduira ensuite, la méthode de Newton, méthode très connue et utilisée dans les problèmes de recherche de zéros. On regardera les avantages et les limites de cette méthode, pour finalement cibler dans quel contexte l'utiliser.
	
	La cinquième partie aura pour objectif d'expliquer les méthodes algorithmiques utilisées pour manipuler les fonctions D-finies sur ordinateur.
	
	Les dernières parties seront une exploitation des outils introduits dans les premières parties pour construire notre algorithme. 
	
	
	\section{Fonctions D-finies}
	Ce chapitre énoncera quelques propriétés des fonctions D-finies sur, de sorte à donner des notions fondamentales au lecteur non familier avec cette famille de fonctions. les démonstration ne seront pas présentés ici, mais sont présentes dans l'article de Stanley d'où elles sont issues.\\
	\\
	On dénotera dans la suite $\mathbb{K}[z]$ les polynômes à une variable sur le corps $\mathbb{K}$, les deux corps que nous verrons ici sont $\mathbb{R}$ et $\mathbb{C}$. On dénotera aussi $\mathbb{K}((z))$ le corps des séries de Laurents à coefficients dans $\mathbb{K}$
	
	\begin{definition}
		Soit $f=\sum_{k\geq n_0}f(n)x^n$ une série de Laurent sur le corps $\mathbb{C}$, f est dite  D-finies si elle satisfait une équation différentielles de la forme
		\begin{equation}
		a_{r}(z)y^{(r)}(z)+a_{n-1}(z)y^{(n-1)}(z)+...+a_{0}(z)y(z)=0, \hspace{3mm} a_{k}\in \mathbb{C}[z].
		\end{equation}
	\end{definition}
	
	
	
 	
 	
	
	
	
	
	\section{Approximation uniforme sur un segment}
	
	Dans cette section, on approchera les fonctions D-finies à l'aide des polynômes.
	Weierstrass a montré qu'il était possible d'approximer sur un segment n'importe quelle fonction continue à partir de polynômes. Cependant, toutes les méthodes d'interpolation ne convergent pas. Un exemple est donné par Runge, la suite de polynômes qui interpolent la fonction $f(x)=\frac{1}{1+x^{2}}$ sur $[-5,5]$ sur N points à distance régulière les uns des autres diverge lorsque $N \rightarrow \infty$.
	
	\subsection{Polynômes de Chebyshev méthode de Boyd}
	
	
	Le mathématicien Russe Sergei Bernstein (1880-1968) a montré que la série de Chebyshev d'une fonction analytique a une convergence quadratique par rapport à la norme infinie. C'est à dire que l'erreur, après avoir tronqué la série après le Nème terme, est $O(exp(-N\mu))$. Dans notre cadre, il est intéressant de vouloir quantifier avec des bornes concrêtes notre approximation.
	C'est cette majoration avec le théorème des valeurs intermédiaires qui nous permettra de certifier les zéros de notre fonction D-finie f.
	
	
	\begin{proposition}
		Soit f une fonction continue, soit g une fonction telle que \\
		$\left\|f-g \right\|_{\infty} \leq \epsilon$. Alors, si il existe a et b tels que $\left\|g(a)\right\| \geq \epsilon$ et $\left\|g(b)\right\| \geq \epsilon$ avec $g(a)g(b) < 0$, alors f s'annule sur l'intervalle [a,b].
		
	\end{proposition}
	
	\subsection{L'algorithme d'approximation}
	
	
	\begin{algorithm}
		\caption{Chebyshev approximation}
		
		\vspace{2mm}
		
		\textbf{Entrée:} fonction D-finie $f(x)=\sum_{n \geq\beta} f(n)x^{n}$. Entier N
		
		\textbf{Sortie:} polynôme d'interpolation g
		
		\begin{algorithmic}[1]
			
			\STATE création des points d'interpolation:$x_{k}=\frac{b-a}{2}cos(\pi\frac{k}{N})+\frac{b+a}{2}$  $k=0,1,..,N$
			\STATE création des points à approximer: $f_{k}=f(x_{k})$ $k=0,1,..,N$
			\STATE création de la matrice d'interpolation M de taille $(N+1)\times (N+1)$:\\
			$p_{j}=2$ $j\in\{1,2\}$ et $p_{j}=1$ sinon, alors:
			$M_{jk}=\frac{2}{p_{j}p_{k}N}cos(j\pi\frac{k}{N})$
			\STATE $a_{j}=\sum_{k=0}^{N}M_{jk}f_{k}$ j=0,1,..,N
			\STATE $g(x)=\sum_{j=0}^{N}a_{j}T_{j}(\frac{2x-(b+a)}{b-a})$
			\STATE Renvoyer g
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent $T_{j}(x)=cos(j$  $arccos(x))$
	
	\noindent On voit ici la limite de cet algorithme, il nécessite une connaissance de la fonction, et c'est justement ce que nous cherchons ici.
	
	
	\subsection{Méthode de Sturm}
	
	Sur un intervalle donné, on veut maintenant voir s'il y a des zéros grâce à l'approximation de Chebyshev et la proposition 1. Nous allons utiliser le principe d'exclusion en comptant le nombre de racines dans l'intervalle à l'aide du théorème de Sturm.
	
	
	Soit P un polynôme unitaire, $P(x)=x^{n} + \sum^{n-1}_{k=0}a_{k}x^{k}$, \textit {la suite de Sturm} est une suite finie de polynômes définie à partir de P comme suit:\\
	$P_{0}=P$, $P_{1}=P'$, et pour $k > 1$ si $P_{k} \neq 0$,  $P_{k+1}$ vérifie
	\[P_{k-1}=P_{k}Q_{k}-P_{k+1},\hspace{2mm}\text{avec }    deg(P_{k+1})<deg(P_{k})\] 
	$P_{k+1}$ est l'opposée du reste dans la division euclidienne de $P_{k-1}$ par $P_{k}$. On a alors le théorème suivant 
	

	\begin{theorem}(Sturm-Habicht)
		Notons $\sigma(\xi)$ le nombre de fois où la suite $P(\xi),P_{1}(\xi),...,P_{m}(\xi)$ change de signe (un zéro ne compte pas comme changement de signe).
		Pour deux réels $a,b$ avec $a<b$ où $a$ et $b$ ne sont pas des racines de P, le nombre de racines dans l'intervalle $[a,b]$ vaut
		$\sigma(a)-\sigma(b)$.
	\end{theorem}
	
	\section{Méthode de Newton}
	
	La méthode de Newton définie par la suite $x_{k+1}=N_{f}(x_{k})$ avec $N_{f}(x)=x-f'(x)^{-1}f(x)$ dans notre cas, nous sommes dans $\mathbb{R}$ alors la méthode de Newton se réécrit $N_{f}(x)=x-\frac{f(x)}{f'(x)}$ 
	
	\subsection{Les applications contractantes}
	
	Ce chapitre se compose de quelques rappels sur les applications contractantes, qui sont la clés de la convergence de la méthode de Newton. En effet, la convergence de cette méthode est assurée lorsque celle-ci est contractancte. 
	Notons $\mathbb{E}$ un espace métrique complet et $d$ sa distance.
	
	
	\begin{definition} Une application $f: \mathbb{E} \rightarrow \mathbb{E}$ est lipschitzienne s'il existe $\lambda \in \mathbb{R}_{+}$ tel que pour tout $x$ et $y in\ \mathbb{E}$ on ait $d(f(x),f'(y)) \leq \lambda$. Une application est dite contractante si elle est lipshitzienne pour une constante $\lambda <1$. 
	\end{definition}
	On peut par ailleurs définir une constante de Lipschitz d'une fonction f sur un segment $[a,b]$
	\[Lip(f)=\sup_{x \neq y,(x,y)\in [a,b]^2} \frac{d(f(x),f(y))}{d(x,y)}\]
	
	On va maintenant donner les théorèmes qui nous seront utiles.
	\begin{theorem}(Théorème des applications conctractantes) Soit $f:\mathbb{E} \rightarrow \mathbb{E}$ une application contractante de constante $0<\lambda<1$.
		
		(a)Pour tout $x_0 \in \mathbb{E}$, la suite $x_{k+1}=f(x_k)$ converge vers un point fixe,\\
		
		(b)Ce point fixe est unique et on le nomme $x$,\\
		
		(c)Pour tout $q \geq 0$, $d(x_q,x) \leq \frac{\lambda^q}{1-\lambda} d(x_0,x_1)$,\\
		
		(d)$\frac{d(x_0,x_1)}{1+\lambda} \leq d(x_0,x) \leq \frac{d(x_0,x_1)}{1-\lambda}$.
	\end{theorem}
	Ce théorème est bien utile, la dernière inégalité permet d'encadrer la distance au zéro, bien utile pour trouver un zéro à une précision $\epsilon$. 
	
	\subsection{Comment vérifier les hypothèses de contraction} 
	
	\subsection{L'algorithme de Newton}
	
	
	
	\begin{algorithm}
		\caption{Newton iteration}
		
		\vspace{2mm}
		
		\textbf{Entrée:} fonction $f(x)$. rationnel a. $\epsilon$ une distance. $\lambda$ la constante de Lipschitz.
		
		\textbf{Sortie:} point b vérifiant $d(b,\zeta)<\epsilon$, où $\zeta$ est la racine. 
		
		\begin{algorithmic}[1]
			\STATE x:=$N_{f}(a)$
			\STATE Tant que $d(x,N_{f}(x))>\frac{\epsilon}{1+\lambda}$\\
					x:=$N_{f}(x)$					
			\STATE return $N_{f}(x)$
		\end{algorithmic}
	\end{algorithm}

	\subsection{Théorie alpha de Smale}
	La théorie alpha de Smale permet d'assurer la présence d'un zéro dans un voisinage, autrement dit de certifier les zéros, les principaux résultats de cette théorie sont rappelés dans la prochaine section.
	Dans cette section les espaces considérés sont des espaces de Banach, $U$ est un ouvert de l'espace. Et les fonctions $f:U\rightarrow \mathbb{F}$ sont analytiques sur U. Nous sommes dans le cas particulier de $\mathbb{R}$ mais cette théorie s'applique sur tout espace métrique complet. 
	
	\begin{definition}
		
		On définit les trois opérateurs $\gamma(f,x)$, $\beta(f,x)$, et $\alpha(f,x)$ sur les fonctions analytiques sur $\mathbb{R}$:
		
		\[\gamma(f,x)=\sup_{k \geq 2}\left\|f'(x)^{-1}\frac{f^{(k)}(x)}{k!}\right\|^{\frac{1}{k-1}}\]
		\[\beta(f,x)=\left\|f'(x)^{-1}f(x)\right\|\]
		\[\alpha(f,x)=\gamma(f,x)\beta(f,x)\]
	\end{definition}
	
	\begin{theorem}(Théorème gamma) Soit $\zeta \in U$ tel que $f(\zeta)=0$ et $f'(\zeta)$ soient inversibles. Soit $x_{0} \in U$ tel que \\
		\[\left\|x_{0}-\zeta\right\|\gamma(f,\zeta) \leq \frac{3-\sqrt{7}}{2}=0.17712.... \]
		Alors la suite de Newton $x_{k+1}=N_{f}(x_{k})$ converge vers $\zeta$. De plus\\
		\[\left\|x_{k}-\zeta\right\| \leq \left(\frac{1}{2}\right)^{n}\left\|x_{0}-\zeta\right\|\]
	\end{theorem}
	
	
	\begin{theorem}(Wang-Han)
		Pour tout $\alpha$ $\in [0,3-2\sqrt{2}]$, la quantité $(1+\alpha^{2})-8\alpha$ décroît de 0 à 1. Posons
		
		\[q=\frac{1-\alpha-\sqrt{(1+\alpha)^{2}-8\alpha}}{1-\alpha+\sqrt{(1+\alpha)^{2}-8\alpha}}\]
		
		On a
		\[0 \leq q<1 \text{ si } 0 \leq a < 3-2\sqrt{2}\]
		\[q=1        \text{ si } 0 \leq a < 3-2\sqrt{2}\]
		Pour tout $x\in U$ tel que $\alpha=\alpha(f,x) \leq 3-2\sqrt{2}$, il existe un et un seul zéro $\zeta$ de $f$ tel que\\
	\end{theorem}
	
	
	\begin{corollaire}
		Pour tout $x\in U$ tel que $\alpha=\alpha(f,x) \leq 3-2\sqrt{2}$, il existe un et un seul zéro $\zeta$ de f tel que
		\[\left\|\zeta-x\right\|\leq\frac{2-\sqrt{2}}{2\gamma(f,x)}\]
		De plus, la suite de Newton $x_{k+1}=N_{f}(x_{k}),$ $x_{0}=x$, est définie et converge vers $\zeta$.
	\end{corollaire}
	\noindent \textbf{Esquisse de démonstration} $2-\sqrt2/2$ est le maximum de $(1+\alpha-\sqrt{(1+\alpha)^{2}-8\alpha})/4$ lorsque $\alpha \in [0,3-2\sqrt2]$.
	\\
	(À compléter)\\
	\\
    Cependant le calcul de $\gamma$ et donc de $\alpha$ est encore difficile pour des coefficients de fonction D-finies vérifiant une récurrence polynomiale. Le chapitre sur les séries majorantes fournie une majoration du coefficient $\alpha$
	
	
	
	
	\section{Majoration des suites P-récursives}
	
	Ce chapitre est quasiment une réécriture du chapitre 5 de la thèse de Marc Mezzarobba. Il majore les suites P-récursives à l'aide du théorème de Perron-Kreuser.
	L'avantage de cette majoration est qu'elle sera fine, c'est à dire qu'on peut quantifier la différence entre la majoration et la solution.
	Cette majoration terme à terme permettra donc de trouver une majoration fine du coefficient alpha et donc des bassins d'attractions donnés par le théorème de (Wang-Han), ce qui permettra à notre algorithme de certifier ses zéros.
	
	Considérons une suite P-récursive u, définie par la récurrence
	\[p^{[0]}(n+s)u_{n}+p^{[1]}(n)u_{n+s-1}+...+p^{[s]}(n)u_{n}=0,\hspace{2mm} p^{[k]}\in \mathbb{Q} \tag{*}\] 
	\begin{theorem}Soit $(u_{n})\in \mathbb{Q}^{\mathbb{N}}$ une suite P-récursive solution de la récurrence homogène (*), avec $p^{[s]}(n) \neq 0$ et $p^{[0]}(n) \neq 0$ pour $n \in \mathbb{N}$. Étant donnée la récurrence (*) et les conditions initiales $u_{0},...,u_{s-1}$, l'algorithme défini dans cette section calcule un réel positif A, un rationnel $\kappa$, un nombre algébrique $\alpha$  et une fonction $\phi$ telle que
		\[\forall n\in \mathbb{N}, \hspace{6mm}|u_{n}| \leq A n!^{\kappa}\alpha^{n}\phi(n)\]
		Avec $\phi(n)=e^{o(n)}$. Pour choix générique des conditions initiales, les paramètres $\kappa$ et $\alpha$ sont optimaux.
	\end{theorem}
	
	La fonction $\phi$ est donnée par une formule explicite, elle-même décrite par un petit nombre de paramètres. Les formes qu'elle peut prendre sont détaillées par la suite.
	
	\subsection{Le théorème de Perron-Kreuser}
	
	On s'intéresse ici au comportement asymptotique des solutions des récurrences. Supposons que les coefficients $b_{n}(n)$ de l'équation
	\[b_{s}(n)u_{n+s}+b_{s-1}(n)u_{n+s-1}+...+b_{s'}(n)u_{n+s'}=0  \tag{*}\]
	(où s' peut être négatif) ont des comportements asymptotiques de la forme
	\[\forall k, \hspace{2mm} b_{k}(n) \sim c_{k}n^{d_{k}} \hspace{2mm} \text{quand } n \rightarrow \infty\]
	avec $c_{k} \in E$ et $d_{k} \in \mathbb{Z}$. Supposons de plus que $u_{n}$ est une solution telle que  
	\[\frac{u_{n+1}}{u_{n}}\sim \lambda n^{\kappa} \hspace{2mm} \text{quand }n \rightarrow \infty\]
	
	\noindent En réécrivant l'équation séquencielle avec ses coefficients asymptotiques $n \rightarrow \infty$
	\[c_{s}\lambda^{s} n^{d_{s}+s\kappa}+c_{s-1}u_{n}\lambda^{s-1} n^{d_{s-1}+(s-1)\kappa}+...+c_{s'}\lambda^{s'} n^{d_{s'}+s'\kappa}u_{n}\]
	pour que cette expression s'annule, il est nécessaire que les termes asymptotiquement dominants se compensent, et donc que l'exposant $d_{k}+k\kappa$ le plus grand, soit atteint au moins deux fois. Alors $-\kappa$ doit être parmi les pentes du \textit{polygone de newton} de l'équation.
	
	
	\begin{definition} Le polygone de Newton est l'enveloppe convexe supérieure des points $(k, d_{k}) \in \mathbb{R}$, si E=[A,B] désigne une arête du polygone de Newton, on note $\kappa(E)$ l'opposé de sa pente, et on définit l'équation caractéristique associée à E (ou à $\kappa(E))$ par
		\[\chi_{E}(\lambda)=\sum_{(k,d_{k}) \in E} c_{k}\lambda^{k-t}\]
		Où $(t,d_{t})=A$ l'extrémité gauche du segment E.
	\end{definition}
	
	Remarquons que la somme des degrés des différentes équations caractéristiques est égale à l'ordre de l'équation de récurrence 
	
	\begin{theorem} (Perron-Kreuser)
		Pour toute arête E du polygone de Newton de la récurrence (*) notons $\lambda_{E_{1}},\lambda_{E_{2}},...$ les racines de $\chi(E)$ comptées avec leur multiplicité.
		
		(a) Supposons que pour toute arête E, les modules $|\lambda_{E_{i}}|$ des racines de $\chi(E)$ sont deux à deux distincts. Alors toute solution non ultimement nulle de (*) satisfait
		\[\frac{u_{n+1}}{u_{n}}\sim \lambda_{E_{i}}n^{\kappa(E)} \hspace{2mm} \text{quand } n \rightarrow \infty\]
		Pour une certaine arête E et un certain i.
		
		(b) Si en outre (*) est réversible, elle admet une base de solution 
		\[(u_{n}^{[E_{i}]})_{E_{i}\leq i \leq deg \chi_{E}}\]
		telle que 
		\[\frac{u_{n+1}^{[E_{i}]}}{u_{n}^{[E_{i}]}}\sim \lambda n^{\kappa(E)} \hspace{2mm} \text{quand } n \rightarrow \infty\]
		
		(c) Dans le cas où il existe E et $i \neq j$ tels que $|\lambda_{E_{i}}|=|\lambda_{E_{j}}|$ les analogues des deux assertions précédentes subsistent mais avec la conclusion plus faible
		\[\limsup_{n \rightarrow \infty } \big|\frac{u_{n}^{E_{i}}}{n!^{\kappa(E)}}\big|^{\frac{1}{n}}=|\lambda_{E_{i}}|\].
	\end{theorem}
	\vspace{7mm}
	Certains résultats sont plus précis dans des cas particuliers et Schäfke et Noble donnent alors une discussion plus précise.
	
	\subsection{Pôles et singularités dominantes}
	
	\begin{definition} Si P $\in \mathbb{Q}[z]$ est un polynôme non réduit à un monôme, on note respectivement
		\[\delta(P)=min\{|\zeta| \neq 0: P(\zeta)=0\} \text{ et } \nu_{\delta}=max\{\nu(\zeta,P):|\zeta|=\delta(P)\}\] 
		On appelle \textit{pôles dominants} d'une fraction rationelle et $\delta$-racines  de son dénominateur, et \textit{singularités dominantes} d'un opérateur différentiel à coefficient polynomiaux celles de son coefficient de tête.  
	\end{definition}
	
	\subsection{Croissance générique des solutions}
	
	\begin{definition} Soit $R \in \mathbb{R}[n]\big< S\big>$ unn opérateur réversible non singulier, d'ordre s. Une solution $(u_{n})$ de la récurrence R.u=0 est alors déterminée de façon unique par ses s premières valeurs. Nous dirons qu'une proposition est vrai pour une solution générique si elle est satisfaite pour $(u_{0},u_{1},...,u_{s-1}) \in \mathbb{R}^{s}/V$ où V est un sous-espace strict de $\mathbb{R}^{s}$.
		
	\end{definition}
	
	\begin{algorithm}
		\caption{Asympt(R)}
		\begin{algorithmic}[1]
			\REQUIRE Un opérateur de récurrence $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{2mm} \in \mathbb{Q}[n]\big<S\big>$
			\vspace{4mm}
			\ENSURE $\kappa \in \mathbb{Q}, P_{\alpha} \in \mathbb{Q}[z].$
			\vspace{4mm}
			\STATE $\kappa:= max_{k=0}^{s-1}\frac{\text{deg } b^{[k]}-\text{deg } b{[s]}}{s-k}$
			\vspace{4mm}
			\STATE $P_{\alpha}:=\sum_{l=0}^{s} b_{d+l\kappa}^{[s-l]}\text{ où } d=\text{deg } b^{[s]}$
			\vspace{4mm}
			\STATE Renvoyer $(\kappa,P_{\alpha})$
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent D'après le théorème de Perron-kreuser les solutions dont "la croissance est la plus rapide", c'est à dire celle dont le coeffficient $\kappa$ est le plus grand est celle la plus à droite du polygone de Newton, et dont la racine est celle de module maximal dans son équation caractéristique.
	L'algorithme renvoie le $\kappa$ maximal et le polynôme réciproque de l'équation caractéristique correspondante au coefficient $\kappa$.
	
	\begin{proposition} Posons $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{4mm} \in \mathbb{Q}[n]\big<S\big>$ et supposons que R n'est pas réduit à un terme $b^{[s]}S^{k}$. On a donc
		\[\limsup_{n \rightarrow \infty }\Big|\frac{u_{n}}{n!^{k}}\Big|^{\frac{1}{n}} \hspace{4mm} \text{où } \alpha=\frac{1}{\delta(P_{\alpha})}\]
		Pour toute solution $(u_{n})$ vérifiant R.u=0 avec égalité pour une solution générique. 
	\end{proposition}
	\textbf{Esquisse de démonstration}
	Le polygone de Newton étant convexe, le coefficient $\kappa$ le plus élevé correspond à un segment relié au sommet le plus à droite du polygone.\\
	Aussi, en écrivant prenant le polynôme caractéristique correspondant, le dérivant par $\lambda^{s}$ et en posant $\beta=\frac{1}{\lambda}$ le polynôme en $\beta$ est le même que celui renvoyé par l'algorithme \textit{Asympt}. Pour ce qui est du rayon de l'inégalité, elle résulte du (c) du théorème de Perron-Kreuser.
	Il reste à démontrer l'inégalité pour des conditions initiales génériques. Soit $V=ker$ $R \subset \mathbb{Q}^{\mathbb{N}}$, d'après le théorème de Perron Kreuser.\\
	Il existe une solution $u^{[0]}$ telle que $\limsup \big|u^{[0]}_{n}/n!^{k}\big|^{1/n}=\alpha$.\\
	Étendons cette solution en une base $(u^{[0]},u^{[1]},..,u^{[s-1]})$ de V. Soit $u=\sum_{k}\lambda^{[k]}u^{[k]}$
	Par construction de $\kappa$ et $\alpha$, on a $\limsup \big|u_{n}/n!^{\kappa}\big|^{1/n} \leq \alpha$. Quitte à extraire des sous-suites pour $u_{n}$ on peut supposer que $u^{[0]}_{n}$ n'est jamais nul, il existe donc $\beta$ tel que
	\[\Big|\lambda^{[0]}+\frac{\lambda^{[1]}u^{[1]}_{n}+...+\lambda^{[s-1]}u^{[s-1]}_{n}}{u^{[0]}_{n}}\Big| \rightarrow_{n \rightarrow \infty} \frac{\beta}{\alpha}\] 
	et $\beta=\alpha$ à moins que 
	\[\frac{\lambda^{[1]}u^{[1]}_{n}+...+\lambda^{[s-1]}u^{[s-1]}_{n}}{u^{[0]}_{n}}\rightarrow_{n \rightarrow \infty}-\lambda^{[0]}\]
	Condition fausse pour des $\lambda^{[k]}$ génériques.
	
	\begin{definition} On appelle arête dominante l'arête la plus à droite du polygone de Newton, équation caractéristique dominante son équation caractéristique associée, et récurence normalisée une récurrence pour laquelle cette arête est horizontale. C'est à dire que le comportement asymptotique est purement exponentiel, et non factoriel ($\kappa=0$).
	\end{definition}
	\vspace{7mm}
	Remarquons que lorsque le degré de tête de l'équation différentielle est de degré supérieur aux autres, ce qui correspond à un arête à droite du polygone telle que $\kappa$ soit positif. Alors la série $\sum_{n\geq 0}u_nx^n$ a un rayon de convergence nul. En fait cela correspond au cas où l'on se trouve sur une singularité. Ce ne sera jamais le cas dans notre étude.
	
	\subsection{Fonction génératrice normalisée}
	
	\begin{algorithm}
		\caption{RecToDiffeq}
		
		\vspace{2mm}
		
		\textbf{Entrée:} $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{4mm} \in \mathbb{Q}[n]\big<S\big>$
		
		\textbf{Sortie:} Un opérateur $D \in \mathbb{Q}[z] \big<\theta\big>$ tel que $\forall (u_{n}) \in \mathbb{Q}^{\mathbb{N}}$ R.u=0 $\iff D.\sum u_{n}x^{n}=0$ 
		
		\begin{algorithmic}[1]
			\vspace{3mm}
			\STATE g:=pgcd$(b^{[s]},\pi)$, où $\pi=\prod_{k=1}^{s}(n+k)$
			\vspace{3mm}
			\STATE calculer les $c_{jk}$ tels que $g.R=\sum_{k=0}^{s}\sum_{j}c_{jk}n^{j}S^{k}$\\
			\vspace{3mm}
			$[\textit{on a ainsi } g.R=\sum_{k=0}^{s}\sum_{j}c_{jk}S^{k}(n-k)^{j}]$
			\vspace{3mm}
			\STATE développer $\sum_{k=0}^{s}\sum_{j} c_{jk}z^{s-k}(\theta-k)^{j}$ sous la forme $D=\sum_{k=0}^{r} a^{[k]}\theta^{k}$
			\vspace{3mm}
			\STATE renvoyer D
		\end{algorithmic}
	\end{algorithm}
	\noindent La multiplication par g assure que les premier termes $u_{0},...,u_{s-1}$
	s'affranchissent des potentielles contraintes crées par les termes indéxés sur les coefficients négatifs. Mais elle ne change pas la relation pour les termes supérieurs à s.
	
	Considérons $R \in \mathbb{Q}[n]\big< S\big>$ un opérateur non singulier, d'ordre s. Une solution $(u_{n})$ de la récurrence R.u=0, u(z) est annulée par l'opérateur RecToDiffeq$(R)(\theta)=\sum_{k=0}^{r}a^{[k]}\theta^{k} \in \mathbb{Q}[z]\big< \theta\big>$. En divisant par $a^{[r]}$ on obtient
	\begin{equation}
	\big(\theta^{r}+\frac{a^{[r-1]}\theta^{r-1}+...+a^{[0]}}{a^{[r]}}\big).u=0
	\end{equation}

	
	\begin{proposition}Si l'opérateur R est normalisé, alors l'origine est un point régulier de D=RecToDiffeq et l'équation caractéristique dominante de $R$ est le polynôme réciproque de $a^{[r]}$ 
	\end{proposition}
	On peut retrouver la démonstration dans la thèse de Marc Mezzarobba.\\
	
	Dans le cas général, on commence par normaliser R.
	Le produit symétrique n'est pas quelconque, regarder Barkatou et al.
	\newpage
	\begin{algorithm}
		\caption{Normalize$(R,\kappa)$}
		
		\vspace{2mm}
		
		\textbf{Entrée:} Un opérateur de récurrence $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \in \mathbb{Q}[n]\big<S\big>$, un rationnel $\kappa$
		
		\textbf{Sortie:} Un opérateur $D \in \mathbb{Q}[x] \big<\theta\big>$ 
		
		\begin{algorithmic}[1]
			\vspace{4mm}
			\STATE $p/q:=\kappa$, avec $p \in \mathbb{Z}$, $q \in \mathbb{N}^{*}$, pgcd$(p,q)=1$
			\vspace{4mm}
			\STATE calculer les coefficients $\hat{b}^{[k]}(n)$ du produit symétrique $\hat{R}=\sum_{k=0}^{qs}\hat{b}^{[k]}(n)S^{k}$\\
			\vspace{2mm}
			de $R$ par $(n+k)^{p}S^{q}-1$
			\vspace{4mm}
			\STATE renvoyer RecToDiffeq($\hat{R}$)
		\end{algorithmic}
	\end{algorithm}

	\begin{proposition}Soit $R \in \mathbb{Q}[n]\big< S\big>$ un opérateur non singulier, réversible de coefficient constant par rapport à S non nul. Soient $p/q, P_{\alpha}$ le comportement asymptotique générique renvoyé par $Asympt$. On suppose que $\delta(P_{\alpha})<\infty$ alors $Normalize(R,p/q)$ calcule un opérateur différentiel D qui annule la série 
		\[u(x)=\sum_{n=0}^{\infty} \psi_{n}u_{n}x^{n}\]
		pour toutes suites $\psi$ et $u$ solutions de
		\[(n+q)^{p}\psi_{n+q}=\psi_{n}\]
		et $R.u=0$, l'opérateur D est régulier à l'origine et le module de sa singularité dominante est égale à $\delta(P_{\alpha})$.
	\end{proposition}
	La démonstration de cette proposition est faite dans la thèse de Marc Mezzarobba, mais il est évident que cette série vérifie une équation de récurrence normalisée, c'est l'objectif de la multiplication par $\psi_n$.
	
	\subsection{Séries majorantes}
	
	\begin{definition} Une série formelle v $\in \mathbb{R}_{+}[[\mathbb{Z}]]$ est appelée série majorante de $ u \in \mathbb{R}[[\mathbb{Z}]$ si v domine u coefficient par coefficient, $\forall n$ $ |u_{n}|\leq v_{n}$. On note $u \unlhd v$.
	\end{definition}
	
	\begin{proposition} Soient $u,u^{[1]},u^{[2]}\in \mathbb{R}[[x]]$, et $v,v^{[1]},v^{[2]} \in \mathbb{R}_{+}[[x]]$ tels que $u\leq v,u^{[1]}\leq v^{[1]}$ et $u^{[2]}\leq v^{[2]}$ alors
		
		(a)Le rayon de convergence de $v$ est inclus dans celui de $u$;
		
		(b)Si $\zeta$ appartient au disque de convergence de $v$, alors $|u(\zeta)|\leq v(|\zeta|)$;
		
		(c)On a les majorations
		\[u' \unlhd v'; \hspace{6mm} u^{[1]}+u^{[2]}\unlhd v^{[1]}+v^{[2]}; \hspace{6mm}u^{[1]}u^{[2]}\unlhd v^{[1]}v^{[2]}; \]
		
		(e)Si $v^{[1]}(0)=0$ alors $ u^{[2]} \circ u^{[1]} \unlhd v^{[2]}\circ v^{[1]}$.
		
	\end{proposition}
	
	\noindent\textbf{Méthode de majoration avec un example simple}\\
	On choisit une série a(z) définie sur $D=\{z,|z|<\rho\}$ soit $u$ une autre série vérifiant $u'(z)=a(z)u(z)$, alors le rayon de convergence de $u$ est le même que celui de a.\\
	La fonction série un rayon de convergence $\rho$ donne à s'intéresser à une majoration de la forme
	\[M(z)=\frac{m}{(1-\rho^{-1} z)^{\kappa}}\] 
	
	\noindent\textbf{Méthode} Pour commencer, $\exists M \in \mathbb{R}_{+}, \forall n$ $|a_{n}|\leq M \rho^{-n}$.
	On écrit la relation sur u en termes de séries: $(n+1)u_{n+1}=\sum_{k=0}^{n}u_{n-j}a_{j}$ en considérant $v$ telle que  $(n+1)v_{n+1}=\sum_{k=0}^{n}v_{n-j}M\rho^{-j}$ on observe $v'(z)=\frac{M}{1-z\rho^{-1}}v(z)$ \\
	On a $v=v_{0}(1-z\rho^{-1})^{-M\rho}$. Si $|u_{0}| \leq v_{0}$ la récurrence permet de conclure $ u\unlhd v$. On sait que le rayon de convergence de $v$ est $\rho$ donc celui de $u$ est supérieur à $\rho$, la condition $u'(z)=a(z)u(z)$ permet de conclure l'égalité.
	
	\subsection{Séries majorantes pour les fonctions D-finies}
	Dans l'exemple que nous avons vu précèdement, les séries majorantes servent à majorer des fonctions, elle sont utilisées sur les restes des séries pour connaître la précision de l'approximation de la fonction étudiée.
	Le cas qui pourrait paraître singulier de l'équation différentielle normalisée est en fait le plus courant puisqu'il correspond au cas où l'on est à distance finie non nulle de la singularité la plus proche.
	
	\subsubsection{restes et résidus}
	\begin{equation}
	P(z,\theta) \cdot u(z)=\big[\theta^{r}p_r(z)+\theta^{r-1}p_{r-1}(z)+...+p_0(z)\big]\cdot u(z)=0
	\label{Eqdiff}
	\end{equation}
	Les polynômes $p_0,p_1,...,p_r$ peuvent être considérés premiers entre eux sans perte de généralité. Soit
	 \[\overset{\sim}{u}(z)=\sum_{n=0}^{N-1}u_n z^n\hspace{2mm}\text{avec}\hspace{2mm}u(z)-\overset{\sim}{u}(z)=R_u^{N}(z)\] 
	alors 
	\[  P(z,\theta) \cdot [u(z)-\overset{\sim}{u}(z)]=P(z,\theta)\cdot \overset{\sim}{u}(z)=q(z)\]
	Où $q(z)$ est de la forme $q_Nz^N+...+q_N+sz^{N+s}$ où $s= \deg_{z}P(z,\theta)$, la raison pour laquelle ce polynôme commence à $N$ vient de l'observation que 
	$P(z,\theta)\cdot R_u^{N}$ commence à N, et qu'il soit de degrés $N+s$ que $P(z,\theta)\cdot\overset{\sim}{u}(z)$ est de degré $N+s$.
	\\
	\subsubsection{l'équation majorante}
	On pose
	\[y(z)=p_r(z)(\overset{\sim}{u}(z)-u(z))\]
	Supposant que $p_r(0) \neq 0$, c'est à dire qu'on soit sur un point régulier, on a alors 
	\[L(z,\theta)\cdot y(z)=\Big[\theta^{r}+\frac{\theta^{r}p_{r-1}(z)+...+p_0(z)}{p_r(z)}\Big]\cdot y(z)=q(z)\]
	On réécrit $L(z,\theta)$ en développant $p_r^{-1}$ en série entière et en réarrangeant les termes
	\[L(z,\theta)=\sum_{j\geq 0} Q_{j}(\theta)z^j.\]
	Comme $p_r(0)\neq 0$, la série entière est à coefficients positifs, donc le polynôme $Q_0$ est de degré $r$, et pour $j \geq 1$, le degré des polynômes est inférieur à $r-1$.
	En repassant l'équation sous forme séquentielle, on a 
	\[L(S^{-1},n)\cdot (y_n)_{n \in \mathbb{Z}}=\sum Q_j(n)S^{-j} \cdot  (y_n)_{n \in \mathbb{Z}} =(q_n)_{n \in \mathbb{Z}} \]
	Alors
	\[y_n=\frac{q_n-\sum_{j\geq1}Q_j(n)y_{n-j}}{Q_0(n)}= \frac{1}{n}\Big[\frac{nq_n}{Q_0(n)}-\sum_{j\geq1}\frac{nQ_j(n)y_{n-j}}{Q_0(n)}\Big]\tag{*}\]
	Avec ce qui a été dit précèdement, les coefficients $nq_n/Q_0(n)$ et $nQ_j(n)/Q_0(n)$ sont bornés.
	Supposons qu'on ait $\hat{q}_n$ et $\hat{a}_n$ bornées aussi telles que
	\begin{equation}
	|nq_n/Q_0(n)|\leq \hat{q}_n,  \hspace{4mm}\forall n\geq n_0
	\label{maj1}
	\end{equation}
	\begin{equation}
	|nQ_j(n)/Q_0(n)|\leq \hat{a}_n,\hspace{3mm}  \forall n\geq n_0, \hspace{2mm} j\geq 1
	\label{maj2}
	\end{equation}
	avec $\hat{a}_j=O(\alpha^j)$ pour un certain $\alpha$ quand $j \rightarrow \infty$. On peut alors déduire de (*)
	\begin{equation}
	y_n\leq|\hat{y}_n|=\frac{1}{n}\Big(\hat{q}_n+\sum_{j\geq1}\hat{a}_{n}\hat{y}_{n-j}\Big)
	\label{majrec}
	\end{equation}
	Dans ce cas, si 
	\begin{equation}
	|y_n|\leq \hat{y}_n \hspace{3mm} \forall n, \hspace{2mm} n<n_0,
	\label{maj3}
	\end{equation}
	
	$\hat{y}_n$ est une série majorante de $y_n$. En traduisant l'équation ~\eqref{majrec} en équation différentielle on obtient
	\begin{equation}
	[\theta-\hat{a}(z)] \hat{y}(z) = \hat{q}(z) \hspace{3mm} \text{où} \hspace{3mm} \hat{a}(z)=\sum_{j \geq 1} \hat{a}_iz^j
	\label{majEqdiff}
	\end{equation}
	$\hat{a}$ est bien définie car $\hat{a}_j=O(\alpha^j)$, $j \rightarrow \infty$. On appelle cette équation, \textit{l'équation de majoration} associée à l'équation différentielle ~\eqref{Eqdiff}\\
	On peut écire une solution générale de l'équation différentielle ~\eqref{majEqdiff} 
	\begin{equation}
	\hat{y}(z)=h(z)\Big(c+\int_{0}^{z}\frac{w^{-1}\hat{q}(w)}{h(w)}\, \mathrm{d}w\Big), \hspace{4mm} h(z)=\exp\Big(\int_{0}^{z}w^{-1}\hat{a}(w)\, \mathrm{d}w\Big)
	\end{equation}
	Où c est un nombre complex. Il faut maintenant choisir les paramètres $\hat{a}$, $\hat{q}$ et c de sorte à ce que les inégalités ~\eqref{maj1},~\eqref{maj2},~\eqref{maj3} soient respectées. On aura alors une série majorante de $R_u^{N}$.
	
	\subsubsection{Séries majorantes dans le cas particulier réel sans singularité}
	
	Dans ce chapitre on se trouve dans le cas particulier de notre problème, nous sommes sur un intervalle $[a,b]$ tel qu'il n'y ait aucune singularité sur cet intervalle. Ce qui signifie vis à vis de l'équation différentielle
	\[P(z,\theta) \cdot u(z)=\big[\theta^{r}p_r(z)+\theta^{r-1}p_{r-1}(z)+...+p_0(z)\big]\cdot u(z)=0 \tag{*}\]
	où les polynômes sont premiers entre eux, que le polynôme $p_r$ ne s'annulle pas sur l'intervalle.\\
	
	Plusieurs familles de fonctions majorantes existent, mais ici ce sont les  fonctions majorantes de la forme $\frac{A}{(1-\alpha z)^\lambda}$ qui sont utilisées pour majorer le reste. Remarquons que $\alpha$ doit vérifier que $\frac{1}{\alpha}$ est supérieure à la distance à la singularité la plus proche de f. C'est aussi égale à $\delta(p_r)$ introduit en section 4.
	Ce chapitre est une explication plus en détails de la méthode de majoration utilisée. Tout d'abord il faut se munir du théorème suivant
	
	\begin{theorem}
		Soient $u$ et $v$ deux fonctions analytiques solutions des équations différentielles
		\[u^{(r)}=a^{[r-1]}u^{(r-1)}+...+a^{[0]}u,\]
		\[v^{(r)}=b^{[r-1]}v^{(r-1)}+...+b^{[0]}v,\]
		pour des fonctions méromorphes $(a^{[i]})_{i=0}^{r-1}$ et $(b^{[i]})_{i=0}^{r-1}$ satisfaisant 
		\[a^{[i]} \unlhd b^{[i]}, \hspace{4mm} i=0...r-1\]
		Si de plus, aucune de ces fonctions a une singularité en 0 et
		\[|u^{(i)}(0)|<v^{(i)}(0),\hspace{4mm} i=0...r-1\] 
		alors $u\unlhd v$.
	\end{theorem}
	Nous avons une méthode permettant de trouver un majorant de façon concrête:\\
	
	(1)Trouver un majorant pour chaque fraction rationnelle 
	\[a^{[i]} \unlhd \frac{M^{[i]}}{(1-\alpha z)^{r-i}}\]
	
	(2)Vérifier que $A/(1-\alpha z)^\lambda$ est une solution de 
	\[y^{(r)}=\sum_{i=0}^{r-1}\frac{M^{[i]}}{(1-\alpha)^{r-i}}y^{(i)}\]
	avec $\lambda$, l'unique racine positive de\[\alpha^{r}\lambda^{\uparrow r}=\sum_{i=0}^{r-1}{M^{[i]}}\alpha^i\lambda^{\uparrow r} \]
	$\lambda^{\uparrow r}=\prod_{i=0}^{r-1}(\lambda+i)$\\
	
	(3)Avec $v(z)=(1-\alpha)^{-\lambda}$, avec le théorème précèdent si l'on choisit
	\[A=\max_{0\leq i\leq r-1}\Big(\frac{v^{(i)}(0)}{f^{(i)}(0)}\Big),\]
	on a
	\[f(z)\unlhd  \frac{A}{(1-\alpha z)^\lambda}\]
	Il n'est pas toujours possible de trouver $\lambda$, mais on peut l'approcher aussi près que l'on veut. 
	\vspace{3mm}
	
	\noindent Si l'on veut trouver une série majorante de $\frac{P(z)}{Q(z)}$ de la forme $M=\frac{1}{(1-\alpha)^m}$ on cherchera 
	\[P(z)\unlhd \frac{M_0}{(1-\alpha)^{m/2}}\]
	et 
	\[\frac{1}{Q(z)}\unlhd \frac{M_1}{(1-\alpha)^{m/2}}\]
	donc 
	\[\frac{P(z)}{Q(z)}\unlhd \frac{M_0M_1}{(1-\alpha z)^{m}}.\]
	Nous n'irons pas plus loins dans l'explication de l'obtention des bornes, mais la démonstration et l'obtention des coefficients sont détaillés dans le rapport de Thomas Grégoire.
	\newpage


	
	\section{Séparation des zéros via la méthode de Rouché}
	Nous allons ici nous intéresser au moyen de certifier nos zéros. On sait que les zéros des fonctions analytiques sont isolés, mais on cherche ici évaluer concrètement la distance entre deux zéros. Cela a été fait dans le livre de Dedieu, et ici nous chercherons à adapter les démonstrations de la théorie alpha de Smale aux séries majorantes.
	
	\begin{definition} Soient $\mathbb{E}$ et $\mathbb{F}$ des espaces de Banach et 
		Soit $f:\mathbb{E}\rightarrow \mathbb{F}$ une fonction analytique, notons $\zeta$ un point de $\mathbb{E}$ tel que $f(\zeta)=0$, alors on note
		\begin{equation}
		 sep(f,\zeta)=\inf_{f(\zeta ')=0, \hspace{0.5mm} \zeta \neq \zeta '}\left\|\zeta-\zeta '\right\|
		\end{equation}
		
	\end{definition}
	Le théorème suivant est le théorème de Rouché, il est général, on l'utilisera pour le cas particulier n=1.

	\begin{theorem}(Rouché) Donnons nous un domaine borné $D \subset \mathbb{C}^{n}$ de frontière de Jordan S et deux application analytiques f et g définies sur un voisinage ouvert de D et à valeurs dans $\mathbb{C}^n$. Si pour tout $z \in S$ 
	\[\left\|f(z)\right\|>\left\|g(z)\right\|\]
	alors $f+g$ admet autant de zéros (comptés avec multiplicités) que f dans D.
	\end{theorem}
	
	
	\subsection{De la théorie alpha de Smale aux séries majorantes}
	
	La suite sera une adaptation de la théorie alpha de Smale aux séries majorantes, l'objectif de cettte partie est de fournir des résultats plus généraux que la théorie de gamma pour n'importe quelle série majorante. L'inconvénient c'est que les conditions "légères" de la théorie alpha seront remplacés par des conditions plus "lourdes". \cite{dedieu2006points}\\
	Nous aurons besoin d'introduire des notations pour parler de la série majorante sur le reste d'une série à l'ordre k.
	
	
	\subsection{Zéros répulsifs}
	On définit trois ensembles pour notre algorithme.
	\begin{definition} Soient les trois ensembles suivants.
		
	$\mathbb{A}_i=\{[c,d], \exists \text{ i zeros comptés avec leur multiplicité dans } [c,d]\}$
	
	$\mathbb{E}=\{[c,d], \not\exists \zeta \in [c,d], f(\zeta)=0\}$
	
	$\mathbb{I}=\{[c,d] \text{ indéterminés }\}$\\
	\end{definition}

	\noindent On cherche à rajouter des intervalles dans l'ensemble $\mathbb{A}_1$, c'est à dire des intervalles contenant un unique zéro de multiplicité 1.
	Avant d'énoncer le théorème, on définit la fonction $S$ dont nous aurons besoin par la suite. 
	\begin{definition} Soit $f$ une fonction définie de $\mathbb{R}$ dans $\mathbb{R}_+$, croissante sur $\mathbb{R}_+$ on définit la fonction $S:(\mathbb{R}_+ \rightarrow \mathbb{R}) \longrightarrow \mathbb{R}_+$  telle que $S(f)$ renvoie le plus petit $x \in \mathbb{R}_+$ tel que $f(x)=1$, elle renvoie 0 si $f(0) \geq1$. On  peut l'écrire formellement
		\begin{equation}
		S(f)=
		\left\lbrace
		\begin{array}{ccc}
		0  & \mbox{si} & f(0) \geq 1\\
		f^{-1}(1)\cap \mathbb{R}_{+} & \mbox{sinon}\\
		\end{array}\right.
		\end{equation}
	\end{definition}
	\vspace{7mm}
	Pour alléger les notations dans la suite on pose $\mathbb{S}_{f}(\zeta)=S\Big(|f'(\zeta)^{-1}|\frac{M^{2}(f,\zeta)(t)}{t}\Big)$\\
	Rappel $\beta(f,\zeta)=|f'(\zeta)^{-1}f(\zeta)|$, c'est aussi la longueur de pas de l'itération de Newton.
	\vspace{7mm}
	\begin{theorem}	Soit f une fonction, $\zeta$ et $\zeta '$ deux points de $\mathbb{R}$ tels que $f'(\zeta)\neq 0$ et que $f(\zeta)=f(\zeta ')=0$ et $|\zeta '-\zeta|$ inférieur au rayon de convergence du développement en série entière de $f$ en $\zeta$. 
		\[Sep(f,\zeta) \geq S\Big(|f'(\zeta)^{-1}|\frac{M^{2}(f,\zeta)(t)}{t}\Big) \hspace{4mm} \]
	\end{theorem}

	\begin{demonstration}Raisonnons par l'absurde, 
	\[f(\zeta ')=f(\zeta)+(\zeta '-\zeta)f'(\zeta)+ \sum_{k \geq 2}\frac{f^{(k)}(\zeta)}{k!}(\zeta'-\zeta)^k\]
	qui se réecrit, en multipliant par $f'(\zeta ')^{-1}$
	\[(\zeta-\zeta')=f'(\zeta)^{-1} \sum_{k \geq 2}\frac{f^{(k)}(\zeta)}{k!}(\zeta'-\zeta)^k\]
	d'où 
	\[1 \leq |f'(\zeta)^{-1}|\sum_{k \geq 2}  \big|\frac{f^{(k)}(\zeta)}{k!}(\zeta'-\zeta)^{k-1} \big|\leq |f'(\zeta)^{-1}| \frac{M^{2}(f,\zeta)(|\zeta-\zeta'|)}{|\zeta-\zeta'|}\]
	\[1 \]
	Or $|\zeta-\zeta'|<S\Big(|f'(\zeta)^{-1}|\frac{M^{2}(f,\zeta)(t)}{t}\Big)=\mathbb{S}_{f}(\zeta)$ la fonction majorante est croisssante alors $1<1$, c'est absurde. Il n'y a donc  pas de zéro autre que $\zeta$  dans l'intervalle $[\zeta-\mathbb{S}_{f}(\zeta),\zeta+\mathbb{S}_{f}(\zeta)]$
	\end{demonstration} 
	\vspace{7mm}
	\noindent Une autre façon de le voir est de développer f en $\zeta$
	\[f(\zeta +t)=0+t\Big(f'(\zeta)+\frac{R^{2}(f,\zeta)(t)}{t}\Big), \]
	si l'on veut pour que ça reste de signe constant on veut donc $|f'(\zeta)|-\frac{M^{2}(f,\zeta)(t)}{t} >0$ ce qui mène à la même conclusion.\\
	Remarque, cette démonstration montre que les zéros sont isolés et borne aussi la distance. Pour un zéro $\zeta$ de multiplicité supérieure, disons m, l'astuce est la même, et $sep(f,\zeta) \geq S\big(\big|\frac{f^{(m)}(\zeta)}{m!}\big|^{-1}\frac{M^{m+1}(f,\zeta)(t)|}{t^{m}}\big)$.
	\subsubsection{Quantification de la distance au zéro}
	Dans cette partie comme dans la précèdente, toutes les séries majorantes vérifient la propriété $[t^1]M^{1}(f,x)(t)=|f'(x)|$.
	\begin{theorem} Soit $f:\mathbb{C} \rightarrow \mathbb{C}$ une fonction analytique, soit $x_0 \in \mathbb{C}$ tel que $f'(x_0) \neq 0$. Alors, quel que soit $r\in [0,\mathbb{S}_{f}(x_0)]$ vérifiant
	\[\beta(f,x)<r-|f'(x_0)|^{-1}M^2(f,x)(r),\]
	il existe un unique zéro pour f sur $B(x_0,r)$ la boule ouverte centrée en $x_0$ de rayon $r$. 
	\end{theorem}
	 Nous montrerons que dans le cas où f est à coefficients réels que $]x_0-r,x_0+r[$ contient un unique zéro. Ce qui vient du fait qu'une racine complexe admet aussi son conjugué comme racine, or la boule trouvée est centrée sur un point de l'axe des réels, donc il devrait y avoir 2 zéros et non pas 1 dans la boule.
	 
	 \vspace{7mm}	
	 
	
	\begin{demonstration}
	
	Définissons $g(x)=f(x)-f(x_0)$, $g(x_0)=0$, $g'(x_0)=f'(x_0)$ d'après le théorème précèdent, $x_0$ est le seul zéro de g dans la boule
	$B(x_0,\mathbb{S}_{f}(x_0))$. La série de Taylor de g est convergente, elle est donnée par
	\[g(x)=f'(x_0)(x-x_0)+ \sum_{k \geq 2}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\]
	Nous allons prouver que si pour r vérifiant $0<r<\mathbb{S}_{f}(x_0)$ on a,
	\[|f'(x_0)^{-1}f(x)-f'(x_0)^{-1}g(x)|=|f'(x_0)^{-1}f(x_0)|<|f'(x_0)^{-1}g(x)|\]
	\\
	Pour tout $x$ avec $|x-x_0|=r$
	\\
	Par le théorème de Rouché $|f'(x_0)^{-1}|f$ et $|f'(x_0)^{-1}|g$ auront le même nombre de zéros dans cette boule. Remarquons que 
	\[|f'(x_0)^{-1}f(x)-f'(x_0)^{-1}g(x)|=|f'(x_0)^{-1}f(x_0)|=\beta(f,x_0)\]
	Aussi
	\[x-x_0=f'(x_0)^{-1}g(x)-f'(x_0)^{-1}\sum_{k \geq 2}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k,\]
	de sorte que
	\[r \leq |f'(x_0)^{-1}g(x)|+|f'(x_0)|^{-1}M^{2}(f,x_0)(r) .\]
	C'est à dire que l'hypothèse de Rouché est satisfaite si
	\[\beta(f,x_0)<r-|f'(x_0)|^{-1}M^{2}(f,x_0)(r)\] 
	
	\end{demonstration}
	Ce sera cette expression qui permettra d'exclure des intervalles. Et de façon générale on fera le calcule dans l'autre sens, on regardera le plus petit r vérifiant l'inégalité et si ce r vérifie la condition $|f'(x_0)^{-1}|\frac{M^{2}(f,\zeta)(r)}{r}\leq 1$, alors on pourra assurer que l'intervalle $[x_0-r,x_0+r]$, contient un unique zéro.
	
	\subsection{Algorithme}
	
	Au vu de ce qui a été démontré précèdement, on peut se rapprocher aussi proche que l'on veut d'un zéro, et si l'on est suffisamment proche et que ce zéro est de multiplicité 1, alors on peut trouver un intervalle dans lequel ce zéro est unique.
	On va commencer par décrire l'algorithme permettant de trouver l'intervalle tel que le zéro soit unique dans cet intervalle.
	On suppose ici que l'on dispose d'un module permettant d'avoir des série majorantes à une précision $\epsilon$ de la plus petite série majorante de la fonction.\\
	D'après ce qui a été vu dans le chapitre 7.2 on peut chercher à trouver le max de la  fonction $T(t)=|f'(x)|t-M^2(f,x)(t)$, on sait que la dérivée est strictement décroissante. Alors, l'endroit où la dérivée s'annule est aussi là où la fonction atteint son maximum, avec ces conditions on peut rechercher le zéro de la dérivée par une méthode dichotomique. Nous avons donc un premier algorithme que nous ne décrirons pas plus, mais qui permet d'approcher $x_0$ où le max est atteint.
	
	
	\begin{algorithm}
		\caption{admis}
		
		\begin{algorithmic}[1]
			\REQUIRE Une fonction D-finie $f(x)=\sum_{n \geq 0} u_nx^{n}$. Un réel $x$. 
			\ENSURE Un intervalle, vide où contenant un unique zéro.
			\STATE $t_0=T'^{-1}(0)$
			\IF {$|f(x)|<T(t_0)$ \textbf{et} $t_0<\mathbb{S}_f(x)<1$}
			\STATE \textbf{Renvoyer}($[x,x+t_0]$)
			\ENDIF
			\STATE \textbf{Renvoyer}($[]$)
		\end{algorithmic}
	\end{algorithm}

	\begin{proposition}
		L'intervalle renvoyé par l'algorithme admis possède un unique zéro, et la fonction est strictement monotone sur cet intervalle.
	\end{proposition}
	\begin{demonstration}
		C'est une conséquence directe de la condition du théorème précèdent.
	\end{demonstration}
	
	Ça a l'intérêt théorique que l'on peut peut se rapprocher aussi proche que l'on veut du zéro par une méthode de dichotomie.

% 	\subsection{Détermination du coefficient de $\gamma(M^1(f,z),z)$ (pas important pour l'instant)}
%	La fonction de majoration de $R^1(f,z)$ renvoyée est donc de la  forme
%	\[h(z)=t|f'(z)|+t^2\frac{A}{(1-\alpha z)^{\lambda}}\]
%	Avec $f'(z)\neq 0$. On rappelle la formule $\gamma(f,z)=max_{k=2}^{N}\left\|f'(z)^{-1}\frac{f^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}$. 
%	On s'intéresse donc à la fonction $g(z)=\frac{A}{(1-\alpha z)^{\lambda}}$, la multiplication par $t^2$ ne fait que décaler les indices des coefficients, on aura donc la formule suivante
%	\[\gamma(M^1(f,z),z)=\sup_{k\geq0}\Big|f'(z)^{-1}\frac{g^{(k)}(z)}{k!}\Big|^{\frac{1}{k+1}}\]
%	\begin{theorem}
%		\[\gamma(M^1(f,z),z)=\max_{k=0}^{\lambda}\Big|f'(z)^{-1}\frac{g^{(k)}(z)}{k!}\Big|^{\frac{1}{k+1}}\]
%	\end{theorem}
%	\begin{demonstration}
%		\noindent Nous nous placerons dans le cas $k\geq 1$, en étudiant l'expression de la fonction de majoration, en rentrant $|f'(z)|^{-1}$ dans la constante $A$. 
%		\[|f'(z)|^{-1}\frac{g^{(k)}}{k!}(z)=\frac{A\alpha^{k}}{(1-\alpha z)^{\lambda+k}}\frac{\lambda^{\uparrow k-1}}{k!}\]
%		Où $\lambda^{\uparrow k}=\prod_{i=0}^{k-1}(\lambda+i)$, On peut encore écrire,
%		\[|f'(z)|^{-1}\frac{g^{(k)}(z)}{k!}=\frac{A\alpha^{k}}{(1-\alpha z)^{\lambda+k}} \binom{\lambda+k-1}{k}\]
%		On a finalement, pour $z < \alpha$,
%		\[\left\|f'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k+1}}= \frac{\alpha}{1-\alpha z} \left\|\frac{A}{\alpha(1-\alpha z)^{\lambda-1}}\binom{\lambda+k-1}{k}\right\|^{\frac{1}{k+1}} \tag{*}\]
%		Considérons cette expression comme une fonction de $\mathbb{R}$ dans $\mathbb{R}$. On pose $K=\frac{A}{\alpha(1-\alpha z)^{\lambda-1}}$
%		\[Q(k)=\frac{K\prod_{i=0}^{k-1}(\lambda+i)}{k!} \text{   bof...}\]
%		\noindent Étudions la fonction $h(x)=f(x)^{\frac{1}{x+1}}$ où f est strictement positive, on peut la réecrire
%		\[h(x)=\text{exp}(\frac{\log f(x)}{x+1}) \]
%		Une étude de signe donne à nous intéresser au signe de l'expression
%		\[(x+1)f'(x)-f(x)\log f(x) \tag{1}\] 
%		qui détermine le signe de la dérivée de h. On regarde donc l'expression \\
%		La démonstration reste à terminer...\\
%	\end{demonstration}
		\section{Une méthode d'exclusion de zéros}
	\begin{definition} Soit f une fonction analytique en x, alors on peut écrire $f(x+t)=\sum_{k \geq 0} \frac{f^{(k)}(x)}{k!}t^{k}$, Le reste de la série à l'ordre k est 
		\[R^{k}(f,x)(t)=\sum_{i \geq k} \frac{f^{(i)}(x)}{i!}t^{i}\]
		et la série majorante de ce reste
		\[M^{i}(f,x)(t) \unrhd R^{i}(f,x)(t).\]
	\end{definition}
	Dans cette partie nous utiliserons aussi la notation $[t^k]f(t)=u_k$, où f est une série entière $f(t)=\sum_{k\geq0}u_kt^k$.
	\subsection{L'algorithme de Bissection-exclusion de J.-C. Yakoubsohn}
	
	Cette partie a pour objectif de proposer un algorithme permettant d'exclure des intervalles dans lesquels on peut certifier qu'il n'y a pas de zéro. Une méthode à été proposée dans $\mathbb{C}$ par J.-C. Yakoubsohn \cite{DBLP:journals/jc/Yakoubsohn05}, nous réutiliserons la fonction introduite dans son article.
	
	\begin{definition}Soit f une fonction analytique en x, alors pour t suffisamment petit\\ $f(x+t)=\sum_{k\geq0} \frac{f^{(k)}(x)}{k!}t^{k}$. On définit $M(f,x)(t)=|f(x)|-\sum_{k\geq1}|\frac{f^{(k)}(x)}{k!}|t^{k}$
	\end{definition}
	\begin{proposition}
		Soit $f$ une fonction analytique, alors $M(f,x)$ vérifie 
		\[\forall t \in \mathbb{R}_+, \hspace{3mm} M(f,x)(t) \leq |f(x+t)|, \hspace{3mm} M(f,x)(t) \leq |f(x-t)|.\]
	\end{proposition}
	Comme $M(f,x)$ est strictement décroissante pour $t>0$, on pose $m(x)=M(f,x)^{-1}(0)$, 
	\begin{definition}
		On définit la fonction $m(x)$ comme l'unique point sur $\mathbb{R}_+$ tel que $M(f,x)(m(x))=0$.
	\end{definition} 
	On peut assurer que $f$ ne s'annule pas sur l'intervalle $]x-m(x),x+m(x)[$.
	La partie négative de $M(f,x)$ peut être remplaçée par n'importe quelle série majorante et encore respecter ces propriétés. C'est ce que nous ferons concrêtement sur ordinateur.\\
	\\
	Plus généralement les fonctions de la forme \[M(f,x)(t)=\big|f(x)+tf'(x)+...+t^{k-1}f^{(k-1)}(x)\big|-M^{k}(f,x)(t)\] où $M^{k}(f,x)$ est la série  majorante du reste à l'orde k en x, vérifie encore les propriétés énoncées précèdement.\\
	
	\noindent L'algorithme suivant donnera une itération de l'algorithme de bissection-exclusion 
	
	
	\begin{algorithm}
		\caption{bissection-exclusion}
		\begin{algorithmic}[1]
			\REQUIRE Une fonction D-finie $f(x)=\sum_{n \geq 0} u_nx^{n}$. Un intervalle $[a,b]$. Un réel strictement positif s
			\ENSURE $a+\frac{s}{2^n}$, tel que $[a,a+\frac{s}{2^n}]$ ne contienne pas de zéro.
			\STATE $g=:M(f,a)$
			\WHILE{$g(s)<0$}
			\STATE s:=s/2
			\ENDWHILE
			\RETURN $a+s$
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent Ici, on n'a qu'une étape de l'algorithme et l'idée sera bien sûr d'appliquer récursivement cette fonction pour avoir des intervalles de plus en plus précis. L'article de Yakoubsohn donne une discussion plus complète sur la taille des intervalles en fonctions des "clusters" de zéros.\\
	
	\subsection{Étude de la convergence de la fonction bissection exclusion}
	On a ici une méthode pour exclure des intervalles, mais peut-on en exclure à une précision arbitraire du zéro? Si oui, alors on pourra se rapprocher suffisament des zéros pour les absorber, c'est ce que nous verrons dans la partie suivante.\\
	Il est démontré que la fonction $m$ est continue dans l'article de Yakoubsohn. 
	D'ailleurs, la décroissance de $M(f,x)$ implique que pour tout $a$ tel que $a<m(x)$, $M(f,x)(a)>0$.\\
	On s'intéresse maintenant à la fonction $M(f,x)$ introduite précèdement, et lorsqu'elle s'annule. C'est important de savoir si les itérations successives de cette méthode permettrons de s'approcher aussi proche du zéro que l'on veut.\\
	\\
	\subsubsection{convergence vers un zéro}
	On va ici démontrer que la fonction permet de se rapprocher aussi proche que l'on veut d'un zéro.
	Il va falloir introduire plusieurs concepts, soit $[a,b]$  le segement sur lequel on travaille, $S_0$ sa longueure.
	On suppose que l'intervalle contient $d$ zéros $z_1,...,z_d$ de $f$ comptés avec leur multiplicité alors on peut écrire
	\[f(z)=\prod_{i=1}^{d}(z-z_i)h(z)\]
	où $h(z)$ est une fonction analytique.
	
	(1)$\forall z \in S_0, \hspace{3mm} \frac{|h^{(k)}(z)|}{k!|h(x)|}\leq \lambda \tau^{k-1}$
	
	(2)2$\tau S_0 \geq \frac{1}{2}$
	
	\subsubsection{Comportement local proche d'un zéro}
	S'intéresse maintenant au comportement locale, proche d'un zéro de cette fonction.
	\begin{theorem}
		Soient f une fonction analytique telle que $f'(x)\neq 0$, et $m(x)$ tel que $M(f,x)(m(x))=0$, alors
		\[\beta(f,x) \geq m(x)\geq\frac{\beta(f,x)}{\beta(f,x)\gamma(f,x)+1}\]
	\end{theorem}
	\begin{demonstration}
		
		Pour commencer, on veut  
		\[M(f,x)(t)=|f(x)|-\sum_{k\geq1}|\frac{f^{(k)}(x)}{k!}|t^{k}=0\]
		On le réecrit en utilisant $\gamma(f,x)=\sup_{k \geq 2}\left\|f'(x)^{-1}\frac{f^{(k)}(x)}{k!}\right\|^{\frac{1}{k-1}}$
		\[|f'(x)|^{-1}M(f,x)(t)=|f'(x)|^{-1}|f(x)|-t-\sum_{k\geq2}|f'(x)^{-1}\frac{f^{(k)}(x)}{k!}|t^{k}\]
		\[|f'(x)|^{-1}M(f,x)(t)\geq \beta(f,x)-t-t\sum_{k\geq2}\gamma(f,x)^{k-1}t^{k-1}\]
		Qui peut encore se réécrire
		\[\beta(f,x)-t-\frac{t^2\gamma(f,x)}{1-\gamma(f,x) t}.\]
		On cherche le point de $\mathbb{R}_+$ tel que cette fonction s'annule et l'on aura une minoration de $m(x)$. En multipliant par $(1-\gamma(f,x) t)$ puis en réarrangeant,
		\[t=\frac{\beta(f,x)}{1+\gamma(f,x) \beta(f,x)}\]
		Donc finalement,
		\[m(x)\geq\frac{\beta(f,x)}{\gamma(f,x) \beta(f,x)+1} \]
	\end{demonstration}
	\vspace{7mm}
	Remarquons que $\gamma$ est une fonction continue donc bornée sur $[a,b]$ (si $f'$ ne s'annule pas sur $[a,b]$) appellons $sup\gamma(f)$ son sup sur l'intervalle étudié. On a donc toujours
	\[m(x)\geq\frac{\beta(f,x)}{\beta(f,x)sup\gamma(f)+1}.\]
	Cette démonstration se généralise à n'importe quelle série majorante et, si l'on conserve le premier terme $|f'(x)|$ dans la série majorante, alors le résultat sera le même en remplaçant $\gamma(f,x)$ par $\gamma(|M^1(f,x),x)$. Dans la suite, nous utiliserons $sup\gamma(M)$, pour montrer que ce résultat est encore vrai pour n'importe quelles séries majorantes telles que 
	\[\sup\gamma(M)=\sup_{x\in[a,b]}\gamma(M^1(f,x),x) \]
	soit défini et $[t^1]M^1(f,x)(t)=|f'(x)|$.
	\\
	Étudions alors la suite $x_k$ vérifiant 
	\begin{equation}
	x_{k+1}=
	\begin{array}{ccc}
	x_k+\frac{\beta(f,x_k)}{\beta(f,x_k)sup\gamma(M^1)+1} & \mbox{si} & f'(x)\neq 0\\
	\end{array}
	\end{equation}
	Nous étudierons la convergence de cette suite. On voit clairement que lorsque $\beta(f,x_k)sup\gamma(M^1)\leq 1$, cette suite converge quadratiquement.
	
	

		
	\section{Algorithme}
	
	L'algorithme générale est finalement
	
	\begin{algorithm}
		\caption{Recherche des zéros simples}
		\begin{algorithmic}[1]
			\REQUIRE Une fonction D-finie $f(x)=\sum_{n \geq 0} u_nx^{n}$. Un intervalle $[a,b]$. Un réel strictement positif s
			\ENSURE  Une liste d'intervalles contenant un unique zéro, tels que la fonction soit strictement monotone sur ces intervalles.
			\STATE Admis:=$[]$
			\STATE Indéterminé:=$a$
			\WHILE{Indéterminé$<b$}
			\IF {admis($f,$ Indéterminé)$=[$Indéterminé$,d] \neq \emptyset$}
				\STATE Admis:=Admis $\cup=[$Indéterminé$,d]$
				\STATE Indéterminés:= $d$  
			\ELSE 
			\STATE Indéterminé=bissection-exclusion($f,a$)
			\ENDIF
			\ENDWHILE
			\RETURN Admis
		\end{algorithmic}
	\end{algorithm}	

	\bibliography{mabib}
	\bibliographystyle{plain}
	
	
\end{document}


