
\documentclass[a4paper,10.5pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[utf8]{inputenc}
\textwidth15cm
\oddsidemargin+0.25cm
\textheight24cm
\topmargin-2cm

\title{Zéros certifiés des fonctions D-finies}
\author{Mathis Deronzier}
\date{}

\begin{document}
	
	\maketitle
	\renewcommand{\contentsname}{Sommaire}
	\newpage
	\tableofcontents
	\newpage
	
	\section{Introduction}
	\newtheorem{theorem}{Théorème}[section] 
	\newtheorem{proposition}{Proposition}
	\newtheorem{corollaire}{Corollaire}
	\newtheorem{definition}{Définition}
	\section{Fonctions D-finies}
	
	Fonctions vérifiants une équation différentielles de la forme
	\[a_{r}(z)y^{(r)}(z)+a_{n-1}(z)y^{(n-1)}(z)+...+a_{0}(z)y(z)=0, \hspace{3mm} a_{k}\in \mathbb{C}[z].\] 
	
	
	
	
	
	\section{Approximation uniforme sur un segment}
	
	Dans cette section on approchera les fonctions D-finies à l'aide des polyonômes.
	Wierstrass a montré qu'il était possible d'approximer sur un segment n'importe quelle fonction continue à partir de polynômes. Cependant, toutes les méthodes d'interpollation de polynômes ne convergent pas. Un exemple est donné par Runge, la suite de polynômes qui interpollent à intervalles égaux la fonction $f(x)=\frac{1}{1+x^{2}}$ sur $[-5,5]$ diverge lorsque $N \rightarrow \infty$.
	
	\subsection{Polynômes de Chebyshev méthode de Boyd}
	
	
	Le mathématicien Russe Sergei Bernstein (1880-1968) a montré que la série de Chebyshev d'une fonction analytique a une convergence quadratique par rapport à la norme infinie. C'est à dire que l'erreur après avoir tronqué la série après le Nème termes est $O(exp(-N\mu))$. On s'intéressera plus tard à une vrai majoration du reste de la série.
	C'est cette majoration avec le théorème des valeurs intermédiaires qui nous permettra de certifier les zéros de notre fonction D-finie f.
	
	
	\begin{proposition}
		Soit f une fonction continue, soit g une fonction telle que \\
		$\left\|f-g \right\|_{\infty} \leq \epsilon$. Alors, si il existe a et b tels que $\left\|g(a)\right\| \geq \epsilon$ et $\left\|g(b)\right\| \geq \epsilon$ avec $g(a)g(b) < 0$, alors f s'annule sur l'intervalle [a,b].
		
	\end{proposition}
	
	\subsection{L'algorithme d'approximation}
	
	
	\begin{algorithm}
		\caption{Chebyshev approximation}
		
		\vspace{2mm}
		
		\textbf{Entrée:} fonction D-finie $f(x)=\sum_{n \geq\beta} f(n)x^{n}$. Entier N
		
		\textbf{Sortie:} polynôme d'interpolation g
		
		\begin{algorithmic}[1]
			
			\STATE création des points d'interpolation:$x_{k}=\frac{b-a}{2}cos(\pi\frac{k}{N})+\frac{b+a}{2}$  $k=0,1,..,N$
			\STATE création des points à approximer: $f_{k}=f(x_{k})$ $k=0,1,..,N$
			\STATE création de la matrice d'interpolation M de taille $(N+1)\times (N+1)$:\\
			$p_{j}=2$ $j\in\{1,2\}$ et $p_{j}=1$ sinon, alors:
			$M_{jk}=\frac{2}{p_{j}p_{k}N}cos(j\pi\frac{k}{N})$
			\STATE $a_{j}=\sum_{k=0}^{N}M_{jk}f_{k}$ j=0,1,..,N
			\STATE $g(x)=\sum_{j=0}^{N}a_{j}T_{j}(\frac{2x-(b+a)}{b-a})$
			\STATE Renvoyer g
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent $T_{j}(x)=cos(j$  $arccos(x))$
	
	\noindent On voit ici la limite de cet algorithme, il necessite une connaissance de la fonction, et c'est justement ce que nous cherchons ici.
	
	\subsection{La méthode des éléments finis}
	
	Une bonne méthode de résolution d'équations différentielles est la méthode des éléments finis. Il faut réflechir à une famille de fonctions qu'il pourrait-être intéressant de prendre ici. La famille usuelle des fonctions en "escalier" ne semble pas être suffisante puisqu'elle ne satisfait la condition $C^{\infty}$.
	La famille des polynômes de Chebyshev semble donc la solution.
	
	\subsection{Méthode de Sturm}
	
	Sur un intervalle donné, on veut maintenant voir si il y a des zéros grace à l'approximation de Chebyshev et la proposition 1. Nous allons utiliser le principe d'exclusion en comptant le nombre de racines dans l'intervalle à l'aide du théorème de Sturm. 
	
	
	Soit P un polynome unitaire, $P(x)=x^{n} + \sum^{n-1}_{k=0}a_{k}x^{k}$, \textit {la suite de Sturm} est une suite finie de polynôme défini à partir de P comme suit:\\
	$P_{0}=P$, $P_{1}=P'$, et pour $k > 1$ si $P_{k} \neq 0$,  $P_{k+1}$ vérifie
	\[P_{k-1}=P_{k}Q_{k}-P_{k+1},\hspace{2mm}\text{avec }    deg(P_{k+1})<deg(P_{k})\] 
	$P_{k+1}$ est l'opposée du reste dans la division euclidienne de $P_{k-1}$ par $P_{k}$. On a alors le théorème suivant 
	

	\begin{theorem}(Sturm-Habicht)
		Notons $\sigma(\xi)$ le nombre de fois où la suite $P(\xi),P_{1}(\xi),...,P_{m}(\xi)$ change de signe (un zéro ne comptant pas comme changement de signe).
		Pour deux réels $a,b$ avec $a<b$ où $a$ et $b$ ne sont pas des racines de P, le nombre de racines dans l'intervalle $[a,b]$ vaut
		$\sigma(a)-\sigma(b)$.
	\end{theorem}
	
	\section{Méthode de Newton}
	
	La méthode de Newton définie par la suite $x_{k+1}=N_{f}(x_{k})$ avec $N_{f}(x)=x-Df(x)^{-1}f(x)$ dans notre cas, nous sommes dans $\mathbb{R}$ alors la méthode de Newton se réécrit $N_{f}(x)=x-\frac{f(x)}{f'(x)}$ 
	
	\subsection{Les applications contractantes}
	
	Ce chapitre se compose de quelques rappels sur les applications contractantes, qui sont la clés de la convergence de la méthode de Newton. En effet, la convergence de cette méthode est assurée lorsque celle-ci est contractancte. 
	Notons $\mathbb{E}$ un espace métrique complet et $d$ sa distance.
	
	
	\begin{definition} Une application $f: \mathbb{E} \rightarrow \mathbb{E}$ est lipschitzienne s'il existe $\lambda \in \mathbb{R}_{+}$ tel que pour tout $x$ et $y in\ \mathbb{E}$ on ait $d(f(x),df(y)) \leq \lambda$. Une application est dite contractante si elle est lipshitzienne pour une constante $\lambda <1$. 
	\end{definition}
	On peut par ailleus définir une constante de lipschitz d'une fonction f sur un segment $[a,b]$
	\[Lip(f)=\sup_{x \neq y,(x,y)\in [a,b]^2} \frac{d(f(x),f(y))}{d(x,y)}\]
	
	On va maintenant donner les théorèmes qui nous seront utiles.
	\begin{theorem}(Théorème des applications conctractantes) Soit $f:\mathbb{E} \rightarrow \mathbb{E}$ une application contractante de constante $0<\lambda<1$.
		
		(a)Pour tout $x_0 \in \mathbb{E}$, la suite $x_{k+1}=f(x_k)$ converge vers un point fixe,\\
		
		(b)Ce point fixe est unique et on le nome $x$,\\
		
		(c)Pour tout $q \geq 0$, $d(x_q,x) \leq \frac{\lambda^q}{1-\lambda} d(x_0,x_1)$,\\
		
		(d)$\frac{d(x_0,x_1)}{1+\lambda} \leq d(x_0,x) \leq \frac{d(x_0,x_1)}{1-\lambda}$.
	\end{theorem}
	Ce théorème est bien utile, surtout la dernière inégalité permettant d'encadrer la distance au zéro, bien utile pour trouver un zéro à une précision $epsilon$. 
	
	\subsection{Comment vérifier les hypothèses de contraction} 
	
	\subsection{L'algorithme de Newton}
	
	
	
	\begin{algorithm}
		\caption{Newton iteration}
		
		\vspace{2mm}
		
		\textbf{Entrée:} fonction $f(x)$. rationel a
		
		\textbf{Sortie:} point b
		
		\begin{algorithmic}[1]
			
			\STATE return $a-\frac{f(a)}{f'(a)}=N_{f}(a)$
		\end{algorithmic}
	\end{algorithm}

	\subsection{Théorie alpha de Smale}
	La théorie alpha de Smale permet d'assurer la présence d'un zéro dans un voisinage, autrement dit de certifier les zéros, les principaux résultats de cette théorie sont rappelés dans la prochaine section.
	Dans cette section les espaces considérés sont des espaces de Banach, $U$ est un ouvert de l'espace. Et les fonctions $f:U\rightarrow \mathbb{F}$ sont analytiques sur U. Nous sommes dans le cas particulier de $\mathbb{R}$ mais cette théorie s'applique sur tout espace métrique complet. 
	
	\begin{definition}
		
		On définit les trois opérateurs $\gamma(f,x)$, $\beta(f,x)$, et $\alpha(f,x)$ sur les fonctions analytiques sur $\mathbb{R}$:
		
		\[\gamma(f,x)=\sup_{k \geq 2}\left\|Df(x)^{-1}\frac{D^{k}f(x)}{k!}\right\|^{\frac{1}{k-1}}\]
		\[\beta(f,x)=\left\|Df(x)^{-1}f(x)\right\|\]
		\[\alpha(f,x)=\gamma(f,x)\beta(f,x)\]
	\end{definition}
	
	\begin{theorem}(Théorème gamma) Soit $\zeta \in U$ tel que $f(\zeta)=0$ et $Df(\zeta)$ soient inversibles. Soit $x_{0} \in U$ tel que \\
		\[\left\|x_{0}-\zeta\right\|\gamma(f,\zeta) \leq \frac{3-\sqrt{7}}{2}=0.17712.... \]
		Alors la suite de Newton $x_{k+1}=N_{f}(x_{k})$ converge vers $\zeta$. De plus\\
		\[\left\|x_{k}-\zeta\right\| \leq \left(\frac{1}{2}\right)^{n}\left\|x_{0}-\zeta\right\|\]
	\end{theorem}
	
	
	\begin{theorem} (Wang-Han)
		Pour tout $\alpha$ $\in [0,3-2\sqrt{2}]$, la quantité $(1+\alpha^{2})-8\alpha$ décroît de 0 à 1. Posons
		
		\[q=\frac{1-\alpha-\sqrt{(1+\alpha)^{2}-8\alpha}}{1-\alpha+\sqrt{(1+\alpha)^{2}-8\alpha}}\]
		
		On a
		\[0 \leq q<1 \text{ si } 0 \leq a < 3-2\sqrt{2}\]
		\[q=1        \text{ si } 0 \leq a < 3-2\sqrt{2}\]
		Pour tout $x\in U$ tel que $\alpha=\alpha(f,x) \leq 3-2\sqrt{2}$, il existe un et un seul zéro $\zeta$ de $f$ tel que\\
	\end{theorem}
	
	
	\begin{corollaire}
		Pour tout $x\in U$ tel que $\alpha=\alpha(f,x) \leq 3-2\sqrt{2}$, il existe un et un seul zéro $\zeta$ de f tel que
		\[\left\|\zeta-x\right\|\leq\frac{2-\sqrt{2}}{2\gamma(f,x)}\]
		De plus, la suite de Newton $x_{k+1}=N_{f}(x_{k}),$ $x_{0}=x$, est définie et converge vers $\zeta$.
	\end{corollaire}
	\noindent \textbf{Esquisse de démonstration} $(2-\sqrt(2)/2$ est le maximum de $(1+\alpha-\sqrt{(1+\alpha)^{2}-8\alpha})/4$ lorsque $\alpha \in [0,3-2\sqrt(2)]$.
	
	(À compléter)
	\noindent Opérateur $\gamma(f,x)$
	
	Ce sera ce dernier théorème qui nous permettra de vérifier si nous avons bien un zéro. Cependant le calcule de $\gamma$ et donc de $\alpha$ sont encore difficiles pour des coefficients de fonctions D-finies vérifiant une récurrence polynomiale. Le chapitre sur les séries majorantes fournie une majoration du coefficient $\alpha$
	
	
	
	
	\section{Majoration des suites P-récursives}
	
	Ce chapitre est quasiment une réécriture du chapitre 5 de la thèse de Marc Mezzarobba. Il majore les suites P-réccursives à l'aide du théorème de Perron-Kreuser.
	L'avantage de cette majoration est qu'elle sera fine, c'est à dire qu'on peut quantifier la différence entre la majoration et la solution.
	Cette majoration terme à terme permettra donc de trouver une majoration fine du coefficient alpha et donc des bassins d'attractions donnés par le théorème de (Wang-Han), ce qui permettra à notre algorithme de certifier ses zéros.
	
	Considérons une suite P-récursive u, définie par la récurrence
	\[p^{[0]}(n+s)u_{n}+p^{[1]}(n)u_{n+s-1}+...+p^{[s]}(n)u_{n}=0,\hspace{2mm} p^{[k]}\in \mathbb{Q} \tag{*}\] 
	\begin{theorem}Soit $(u_{n})\in \mathbb{Q}^{\mathbb{N}}$ une suite P-récursive solution de la récurrence homogène (*), avec $p^{[s]}(n) \neq 0$ et $p^{[0]}(n) \neq 0$ pour $n \in \mathbb{N}$. Étant donnée la récurrence (*) et les conditions initiales $u_{0},...,u_{s-1}$, l'algorithme définit dans cette section calcule un réel positif A, un rationnel $\kappa$, un nombre algébrique $\alpha$  et une fonction $\phi$ tels que
		\[\forall n\in \mathbb{N}, \hspace{6mm}|u_{n}| \leq A n!^{\kappa}\alpha^{n}\phi(n)\]
		Avec $\phi(n)=e^{o(n)}$. Pour choix générique des conditions initiales, les paramètres $\kappa$ et $\alpha$ sont optimaux.
	\end{theorem}
	
	La fonction $\phi$ est donnée par une formule explicite, elle-même décrite par un petit nombre de paramètres. Les formes qu'elle peut prendre sont détaillées par la suite.
	
	\subsection{Le théorème de Perron-Kreuser}
	
	On s'interesse ici au comportement asymptotique des solutions des récurrences. Supposons que les coefficients $b_{n}(n)$ de l'équation
	\[b_{s}(n)u_{n+s}+b_{s-1}(n)u_{n+s-1}+...+b_{s'}(n)u_{n+s'}=0  \tag{*}\]
	(où s' peut être négatif) ont des comportements asymptotiques de la forme
	\[\forall k, \hspace{2mm} b_{k}(n) \sim c_{k}n^{d_{k}} \hspace{2mm} \text{quand } n \rightarrow \infty\]
	avec $c_{k} \in E$ et $d_{k} \in \mathbb{Z}$. Supposons de plus que $u_{n}$ est une solution de la forme  
	\[\frac{u_{n+1}}{u_{n}}\sim \lambda n^{\kappa} \hspace{2mm} \text{quand}n \rightarrow \infty\]
	
	\noindent En réécrivant l'équation séquencielle avec ses coefficients asymptotiques $n \rightarrow \infty$
	\[c_{s}\lambda^{s} n^{d_{s}+s\kappa}+c_{s-1}u_{n}\lambda^{s-1} n^{d_{s-1}+(s-1)\kappa}+...+c_{s'}\lambda^{s'} n^{d_{s'}+s'\kappa}u_{n}\]
	
	pour que cette expression s'annule, il est nécessaire que les termes asymptotiquement dominants se compense, et donc que l'exposant $d_{k}+k\kappa$ le plus grand soit atteint au moins deux fois. Alors $-\kappa$ doit être parmis les pentes des tes du \textit{polygone de newton} de l'équation.
	
	
	\begin{definition} Le polygône de Newton est l'envellope convexe supérieure des points $(k, d_{k}) \in \mathbb{R}$, si E=[A,B] désigne une arête du polygône de Newton, on note $\kappa(E)$ l'opposée de sa pente, et on définit l'équation caractéristique associée à E (ou à $\kappa(E))$ par
		\[\chi_{E}(\lambda)=\sum_{(k,d_{k}) \in E} c_{k}\lambda^{k-t}\]
		Où $(t,d_{t})=A$ l'extrémité gauche du segment E.
	\end{definition}
	
	Remarquons que la somme des degrés des différentes équations caractéristiques est égale à l'ordre de l'équation de récurrence 
	
	\begin{theorem} (Perron-Kreuser)
		Pour toute arête E du polygone de Newton de la récurrence (*) notons $\lambda_{E_{1}},\lambda{E_{2}},...$ les racines de $\chi(E)$ comptées avec leur multiplicité.
		
		(a) supposons que pour toute arête E, les modules $|\lambda_{E_{i}}|$ des racines de $\chi(E)$ sont deux à deux distincts. Alors toute solution non ultimement nulle de (*) satisfait
		\[\frac{u_{n+1}}{u_{n}}\sim \lambda_{E_{i}}n^{\kappa(E)} \hspace{2mm} \text{quand } n \rightarrow \infty\]
		Pour une certaine arête E et un certain i.
		
		(b)Si en outre (*) est réversible, elle admet une base de solution 
		\[(u_{n}^{[E_{i}]})_{E_{i}\leq i \leq deg \chi_{E}}\]
		telle que 
		\[\frac{u_{n+1}^{[E_{i}]}}{u_{n}^{[E_{i}]}}\sim \lambda n^{\kappa(E)} \hspace{2mm} \text{quand } n \rightarrow \infty\]
		
		(c)Dans le cas où il existe E et $i \neq j$ tels que $|\lambda_{E_{i}}|=|\lambda_{E_{j}}|$ les analogues des deux assertions précédente subsistent mais avec la conclusion plus faible
		\[\limsup_{n \rightarrow \infty } \big|\frac{u_{n}^{E_{i}}}{n!^{\kappa(E)}}\big|^{\frac{1}{n}}=|\lambda_{E_{i}}|\]
	\end{theorem}
	
	Certains résultats sont plus précis dans des cas particuliers dans le Schäfke et Noble.
	
	\subsection{Esquisse de l'algorithme}
	
	On commence par étudier le polygone de Newton de la récurrence et les équations caractéristiques de la récurrence, pour délimiter à l'aide du théorème de Perron-Kreuser les comportements asymptotiques possibles de $u_{n}$.
	
	\subsection{Pôles et singularités dominantes}
	
	\begin{definition} Si P $\in \mathbb{Q}[z]$ est un polynôme non réduit à un monôme, on note respectivement
		\[\delta(P)=min\{|\zeta| \neq 0: P(\zeta)=0\} \text{ et } \nu_{\delta}=max\{\nu(\zeta,P):|\zeta|=\delta(P)\}\] 
		On appelle \textit{pôles dominants} d'une fraction rationelle et $\delta$-racines  de son dénominateur, et \textit{singularités dominantes} d'un opérateur différentielle à coefficient polynomiaux celles de son coefficient de tête.  
	\end{definition}
	
	\subsection{Croissance générique des solutions}
	
	\begin{definition} Soit $R \in \mathbb{Q}[n]\big< S\big>$ unn opérateur réversible non singulier, d'ordre s. Une solution $(u_{n})$ de la récurrence R.u=0 est alors déterminée de façon unique par ses s premières valeurs. Nous dirons qu'une proposition est vrai pour une solution générique si elle est satisfaite pour $(u_{0},u_{1},...,u_{s-1}) \in \mathbb{Q}^{s}/V$ où V est un sous-espace strict de $\mathbb{Q}^{s}$
		
	\end{definition}
	
	\begin{algorithm}
		\caption{Asympt(R)}
		
		\vspace{2mm}
		
		\textbf{Entrée:} $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{4mm} \in \mathbb{Q}[n]\big<S\big>$
		
		\textbf{Sortie:} $\kappa \in \mathbb{Q}, P_{\alpha} \in \mathbb{Q}[z].$
		
		\begin{algorithmic}[1]
			\vspace{4mm}
			\STATE $\kappa:= max_{k=0}^{s-1}\frac{\text{deg } b^{[k]}-\text{deg } b{[s]}}{s-k}$
			\vspace{4mm}
			\STATE $P_{\alpha}:=\sum_{l=0}^{s} b_{d+l\kappa}^{[s-l]}\text{ où } d=\text{deg } b^{[s]}$
			\vspace{4mm}
			\STATE Renvoyer $(\kappa,P_{\alpha})$
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent D'après le théorème de Perron-kreuser les solutions dont "la croissance est la plus rapide", c'est à dire celle dont le coeffficient $kappa$ est le plus grand est celle la plus à droite du polygone de Newton, et les racines de module maximal de son équation caractéristique.
	
	l'algorithme renvoie le $kappa$ maximale et le polynôme réciproque de l'équation caractéristique correspondante au coefficient $\kappa$.
	
	\begin{proposition} Posons $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{4mm} \in \mathbb{Q}[n]\big<S\big>$ et supposons que R n'est pas réduit à un terme $b^{[s]}S^{k}$. On a donc
		\[\limsup_{n \rightarrow \infty }\Big|\frac{u_{n}}{n!^{k}}\Big|^{\frac{1}{n}} \hspace{4mm} \text{où } \alpha=\frac{1}{\delta(P_{\alpha})}\]
		Pour toute solution $(u_{n})$ vérifiant R.u=0 avec égalité pour une solution générique. 
	\end{proposition}
	\textbf{Esquisse de démonstration}
	Le polygone de Newton étant convexe, le coefficient $\kappa$ le plus élevé correspond à un segment relié au sommet le plus à droite du polygone.\\
	Aussi, en écrivant prenant le polynôme caractéristique correspondant, le dérivant par $\lambda^{s}$ et en posant $\beta=\frac{1}{\lambda}$ le polynôme en $\beta$ est le même que celui renvoyé par l'algorithme \textit{asympt}. Pour ce qui est du rayon de l'inégalité, elle résulte du (c) du théorème de Perron-Kreuser.
	Il reste à démontrer l'inégalité pour des conditions initiales génériques. Soit $V=ker$ $R \subset \mathbb{Q}^{\mathbb{N}}$, d'après le théorème (vrai?????)\\
	Il existe une solution $u^{[0]}$ telle que $\limsup \big|u^{[0]}_{n}/n!^{k}\big|^{1/n}=\alpha$.\\
	Étendons cette solution en une base $(u^{[0]},u^{[1]},..,u^{[s-1]})$ de V. Soit $u=\sum_{k}\lambda^{[k]}u^{[k]}$
	Par construction de $\kappa$ et $\alpha$, on a $\limsup \big|u_{n}/n!^{\kappa}\big|^{1/n} \leq \alpha$. Quitte à extraire des sous-suites pour $u_{n}$ on peut supposer que $u^{[0]}_{n}$ n'est jamais nul,il existe donc $\beta$ tel que
	\[\Big|\lambda^{[0]}+\frac{\lambda^{[1]}u^{[1]}_{n}+...+\lambda^{[s-1]}u^{[s-1]}_{n}}{u^{[0]}_{n}}\Big| \rightarrow_{n \rightarrow \infty} \frac{\beta}{\alpha}\] 
	et $\beta=\alpha$ à moins que 
	\[\frac{\lambda^{[1]}u^{[1]}_{n}+...+\lambda^{[s-1]}u^{[s-1]}_{n}}{u^{[0]}_{n}}\rightarrow_{n \rightarrow \infty}-\lambda^{[0]}\]
	Condition fausse pour des $\lambda^{[k]}$ génériques.
	
	\begin{definition} On appelle arête dominante l'arête la plus à droite du polygone de Newton, équation caractéristique dominante son équation caractéristique associée, et réccurence normalisé une récurrence pour laquelle cette arête est horizontale. C'est à dire que le comportement asymptotique est purement exponentiel, et non factoriel ($\kappa=0$).
	\end{definition}
	
	\subsection{Fonction génératrice normalisée}
	
	\begin{algorithm}
		\caption{RecToDiffeq}
		
		\vspace{2mm}
		
		\textbf{Entrée:} $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \hspace{4mm} \in \mathbb{Q}[n]\big<S\big>$
		
		\textbf{Sortie:} Un opérateur $D \in \mathbb{Q}[z] \big<\theta\big>$ tel que $\forall (u_{n}) \in \mathbb{Q}^{\mathbb{N}}$ R.u=0 $\iff D.\sum u_{n}x^{n}=0$ 
		
		\begin{algorithmic}[1]
			\vspace{3mm}
			\STATE g:=pgcd$(b^{[s]},\pi)$ où $\pi=\prod_{k=1}^{s}(n+k)$
			\vspace{3mm}
			\STATE calculer les $c_{jk}$ tels que $g.R=\sum_{k=0}^{s}\sum_{j}c_{jk}n^{j}S^{k}$\\
			\vspace{3mm}
			$[\textit{on a ainsi } g.R=\sum_{k=0}^{s}\sum_{j}c_{jk}S^{k}(n-k)^{j}]$
			\vspace{3mm}
			\STATE développer $\sum_{k=0}^{s}\sum_{j} c_{jk}z^{s-k}(\theta-k)^{j}$ sous la forme $D=\sum_{k=0}^{r} a^{[k]}\theta^{k}$
			\vspace{3mm}
			\STATE renvoyer D
		\end{algorithmic}
		
	\end{algorithm}
	La multiplication par g s'assure que les premier termes $u_{0},...,u_{s-1}$
	s'affranchissent des potentielles contraintes crées par les termes indéxés sur les coefficients négatifs. Mais ne change pas la relation pour les termes supérieurs à s.
	
	Considérons $R \in \mathbb{Q}[n]\big< S\big>$ un opérateur non singulier, d'ordre s. Une solution $(u_{n})$ de la récurrence R.u=0, u(z) est annulée par l'opérateur RecToDiffeq$(R)(\theta)=\sum_{k=0}^{r}a^{[k]}\theta^{k} \in \mathbb{Q}[z]\big< \theta\big>$. En divisant par $a^{[r]}$ on obtient
	
	\[\big(\theta^{r}+\frac{a^{[r-1]}\theta^{r-1}+...+a^{[0]}}{a^{[r]}}\big).u=0\]
	
	\begin{proposition}Si l'opérateur R est normalisé, alors l'origine est un point régulier de D=RecToDiffeq et l'équation caractéristique dominante de $R$ est le polynôme réciproque de $a^{[r]}$ 
	\end{proposition}
	
	Dans le cas général, on commence par normaliser R.
	Le produit symétrique n'est pas quelconque, regarder Barkatou et al.
	\begin{algorithm}
		\caption{Normalize$(R,\kappa)$}
		
		\vspace{2mm}
		
		\textbf{Entrée:} Un opérateur de récurrence $R=\sum_{k=0}^{s} b^{[k]}(n)S^{k} \in \mathbb{Q}[n]\big<S\big>$, un rationnel $\kappa$
		
		\textbf{Sortie:} Un opérateur $D \in \mathbb{Q}[x] \big<\theta\big>$ 
		
		\begin{algorithmic}[1]
			\vspace{4mm}
			\STATE $p/q:=\kappa$, avec $p \in \mathbb{Z}$, $q \in \mathbb{N}^{*}$, pgcd$(p,q)=1$
			\vspace{4mm}
			\STATE calculer les coefficients $\hat{b}^{[k]}(n)$ du produit symétrique $\hat{R}=\sum_{k=0}^{qs}\hat{b}^{[k]}(n)S^{k}$\\
			\vspace{2mm}
			de $R$ par $(n+k)^{p}S^{q}-1$
			\vspace{4mm}
			\STATE renvoyer RecToDiffeq($\hat{R}$)
		\end{algorithmic}
		
	\end{algorithm}
	
	
	\begin{proposition}Soit $R \in \mathbb{Q}[n]\big< S\big>$ un opérateur non singulier, réversible de coefficient constant par rapport à S non nul. Soient $p/q, P_{\alpha}$ le comportement asymptotique génériquee renvoyé par $Asympt$. On suppose que $\delta(P_{\alpha})<\infty$ alors $Normalize(R,p/q)$ calcule un opérateur différentiel D qui annule la série 
		\[u(x)=\sum_{n=0}^{\infty} \psi_{n}u_{n}x^{n}\]
		pour toutes suites $\psi$ et $u$ solutions de
		\[(n+q)^{p}\psi_{n+q}=\psi_{n}\]
		et $R.u=0$, l'opérateur D est régulier à l'origine et le module de sa singularité dominante est égale à $\delta(P_{\alpha})$.
	\end{proposition}
	
	\subsection{Séries majorantes}
	
	\begin{definition} Une série formelle v $\in \mathbb{R}_{+}[[\mathbb{Z}]]$ est appelée série majorante de $ u \in \mathbb{R}[[\mathbb{Z}]$ si v domine u coefficient par coefficient, $\forall n$ $ |u_{n}|\leq v_{n}$. On note $u \unlhd v$.
	\end{definition}
	
	\begin{proposition} Soient $u,u^{[1]},u^{[2]}\in \mathbb{R}[[x]]$, et $v,v^{[1]},v^{[2]} \in \mathbb{R}_{+}[[x]]$ tels que $u\leq v,u^{[1]}\leq v^{[1]}$ et $u^{[2]}\leq v^{[2]}$ alors
		
		(a)Le rayon de convergence de $v$ est inclus dans celui de $u$;
		
		(b)Si $\zeta$ appartient qu disque de convergence de $v$, alors $|u(\zeta)|\leq v(|\zeta|)$;
		
		(c)On a les majorations
		\[u' \unlhd v'; \hspace{6mm} u^{[1]}+u^{[2]}\unlhd v^{[1]}+v^{[2]}; \hspace{6mm}u^{[1]}u^{[2]}\unlhd v^{[1]}v^{[2]}; \]
		
		(e)Si $v^{[1]}(0)=0$ alors $ u^{[2]} \circ u^{[1]} \unlhd v^{[2]}\circ v^{[1]}$.
		
	\end{proposition}
	
	\noindent\textbf{Méthode de majoration avec un example simple}\\
	On choisit une série a(z) définie sur $D=\{z,|z|<\rho\}$ soit $u$ une autre série vérifiant $u'(z)=a(z)u(z)$ alors le rayon de convergence de $u$ est le même que celui de a.\\
	La fonction série un rayon de convergence $\rho$ donne à s'intéresser à une majoration de la forme
	\[M(z)=\frac{m}{(1-\rho^{-1} z)^{\kappa}}\] 
	
	\noindent\textbf{Méthode} Pour commencer, $\exists M \in \mathbb{R}_{+}, \forall n$ $|a_{n}|\leq M \rho^{-n}$.
	On écrit la relation sur u en terme de séries: $(n+1)u_{n+1}=\sum_{k=0}^{n}u_{n-j}a_{j}$ en considérant $v$ telle que  $(n+1)v_{n+1}=\sum_{k=0}^{n}v_{n-j}M\rho^{-j}$ on observe $v'(z)=\frac{M}{1-z\rho^{-1}}v(z)$ \\
	On a $v=v_{0}(1-z\rho^{-1})^{-M\rho}$. Si $|u_{0}| \leq v_{0}$ la récurrence permet de conclure $ u\unlhd v$. On sait que le rayon de convergence de $v$ est $\rho$ donc celui de $u$ est supérieur à $\rho$, la condition $u'(z)=a(z)u(z)$ permet de conclure l'égalité.
	
	\subsection{Séries majorantes pour les fonctions D-finies normalisées}
	
	Soit D une équation différentielle normalisée, l'intérêt de cette normalisation est de se débarasser du $\kappa$ pour ne plus avoir de comportement asymptotique factoriel de la suite $u_n$. L'idée est la même que dans la méthode exposée précèdement, on cherche à trouver une récurrence qui pourra assurer que si les premiers coefficients sont bien majorés, alors la suite entière est majorée.
	
	\[\theta^{r}+\frac{a^{[r-1]}\theta^{r-1}+...+a^{[0]}}{a^{[r]}}\]
	Le procédé de majoration est le suivant. En isolant le coefficient constant du développement de Taylor en zéro de chaque coefficient, l'équation se réecrit
	\[Q(\theta).u=z(a^{[r-1]}\theta^{r-1}+...+a^{[0]}).u\]
	
	
	\newpage
	
	
	
	\section{Étude de la fonction de majoration et détermination d'un majorant de alpha}
	La fonction de majoration renvoyée par la fonction Bound NormalDiffeq$(D,P_{\alpha},u)$  est de la  forme
	\[g(z)=\frac{A}{(1-\alpha z)^{K}} \hspace{2mm}\text{si T=0}\hspace{6mm} g(z)=\text{A exp}(\frac{K/T}{(1-\alpha z)^{T}}) \hspace{2mm} \text{sinon.} \]
	
	\noindent L'objectif ici est de borner le coefficient $\gamma(f,z)=\sup_{k \geq 2} \left\|f'(z)^{-1}\frac{f^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}$ en utilisant la fonction majorante développée dans la partie précèdente. Elle respecte la propriété interessante $f\unlhd g$ et donc trouver $\alpha(g,z)$ sur cette fonction reviendrai à majorer $\alpha(f,z)$. L'objectif serait de trouver N à partir du quel $\left\|g'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}$ est décroissante. Alors 
	\[\gamma(g,z)=max_{k=2}^{N}\left\|g'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}\] 
	
	\begin{proposition} Dans le cas $T=0$ on a \[\gamma(g,z)=max_{k=2}^{\lceil e^{1}K!^{\frac{1}{K-1}} \rceil}\left\|g'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}\]
	\end{proposition}

	\noindent\textbf{Démonstration}
	\noindent En étudiant l'expression de la fonction de majoration pour $T=0$ on obtient
	\[g^{(k)}(z)=\frac{A\alpha^{k}\prod_{i=0}^{k-1}(K+i)}{(1-\alpha z)^{K+k}}\]
	On peut écrire,
	\[\frac{g^{(k)}(z)}{k!}=\frac{A\alpha^{k}}{(1-\alpha z)^{K+k}} \frac{\prod_{i=1}^{K-1}(k+i)}{(K-1)!} \]
	On a finalement, pour $z < \alpha$
	\[\left\|g'(z)^{-1}\frac{g^{(k)}(z)}{k!}\right\|^{\frac{1}{k-1}}= \frac{\alpha}{1-\alpha z} \left\|\frac{\prod_{i=1}^{K-1}(k+i)}{K!}\right\|^{\frac{1}{k-1}} \tag{*}\]
	Considérons cette expression comme une fonction de $\mathbb{R}$ dans $\mathbb{R}$. On a donc un polynôme de degré $K-1$.
	\[P(k)=\frac{\prod_{i=1}^{K-1}(k+i)}{K!}\]
	
	
	\noindent Étudions la fonction $h(x)=(f(x))^{\frac{1}{x-1}}$ où f est strictement positive, on peut la réecrire
	\[h(x)=\text{exp}(\frac{ln(f(x))}{x-1}) \]
	Une étude de signe donne à nous intéresser au signe de l'expression
	\[(x-1)f'(x)-f(x)ln(f(x)) \tag{1}\] 
	qui détermine le signe de la dérivée de h. On regarde donc l'expression 
	\[(k-1)P'(k)-P(k)ln(P(k))=P(k)\Big[\big(\sum_{i=1}^{K-1}\frac{k-1}{k+i}\big)-ln(P(k))\Big]\]
	On majore $\frac{k-1}{k+i}$ par 1 et minore $P(k)$ par $\frac{k^{K-1}}{K!}$ On est sûr que la série est décroissante lorsque 
	\[K-1-ln\big(\frac{k^{K-1}}{K!}\big)<0 \Longleftrightarrow e^{1}K!^{\frac{1}{K-1}}< k\]
	On sait alors que 
	\[\gamma(f,z)=max_{k=2}^{\lceil e^{1}K!^{\frac{1}{K-1}} \rceil}\left\|f'(z)^{-1}\frac{f^{(k)}(z)}{k!}\right\| \]
	
	\vspace{1cm} 
	
	On s'intéresse maintenant au cas où $T\neq 0$ de l'expression, on veut développer $f(z)$ dans le cas où $T \neq 0$. On ne s'en est pas préoccupé précèdement, mais ici, la constante A ne servira pas dans la majoration, qui sera absorbée dans division par $f'(z)$. En posant \[\alpha_{k}(z)=\frac{\big(\frac{1}{(1-\alpha z)^{T}}\big)}{k!}^{(k)}=\frac{\alpha^{k}}{(1-\alpha z)^{T+k}} \frac{\prod_{i=1}^{T-1}(k+i)}{(K-1)!},\] On a,
	\[g(z+t)=\text{A exp}(\frac{K/T}{(1-\alpha (z+t))^{T}})\]
	En intégrant ce qui a été dit précèdement et en développant en série entière à l'intérieur d'exponentiel on s'intéressera à
	\[f(z+t)=\text{exp}\Big(K/T \sum_{k=1}^{+\infty}\alpha_{k}(z)t^{k} \Big)\]
	
	\[= 1+(K/T)\frac{\sum_{k=1}^{+\infty}\alpha_{k}(z)t^{k}}{1!}+(K/T)^{2}\frac{\big(\sum_{k=1}^{+\infty}\alpha_{k}(z)t^{k}\big)^{2}}{2!}+...+(K/T)^{l}\frac{\big(\sum_{k=1}^{+\infty}\alpha_{k}(z)t^{k}\big)^{l}}{l!}+...\]
	En identifiant les coefficients correspondants à $t^{k}$
	\[\frac{f^{(k)}(z)}{k!}=\sum_{i=1}^{k}\frac{(K/T)^{i}}{i!}\Big(\sum_{r_{1}\leq r_{2}\leq ...\leq{r_{i}}}\Pi(r_{1},...,r_{i})\prod_{h=1}^{i}\alpha_{r_{h}}(z) \Big) \hspace{2mm} \text{avec }\sum_{h=1}^{i} r_{h}=k\]
	Où $\Pi(r_{1},...,r_{i})$ est définie comme suit:\\
	On regroupe les $r_{h}$ en $g$ groupes $\Gamma$ de coefficients égaux, soit $\Gamma_{h}$ leur taille telle que $\sum_{h=1}^{g}\Gamma_{h}=i$, alors
	\[\Pi(r_{1},r_{2},...,r_{i})=\dbinom{i}{\Gamma_{g}}\times \dbinom{i-\Gamma_{g}}{\Gamma_{g-1}}\times ...\times \dbinom{\Gamma_{1}}{\Gamma_{1}}=\frac{i!}{\Gamma_h! \times \Gamma_{h-1}! \times ... \times \Gamma_1!}\]
	
	
	
	\section{Les méthodes d'exclusion de zéros}
	\subsection{L'algorithme de Bissection-exclusion de J.-C.Yakoubsohn}
	L'idée fondamentale de l'algorithme de Jakoubsohn est de constuire une fonction $M(f,x)$ strictement décroissante sur $\mathbb{R}_+$ telle que si cette fonction ne s'annule pas sur un intervalle $[x-\alpha,x+\alpha]$, on peut assurer que la fonction f ne s'annule pas.
	
	\begin{definition}Soit f une fonction analytique en x, alors pour t suffisamment petit\\ $f(x+t)=f(x)+\sum_{k=1}^{+\infty} \frac{f^{(k)}(x)}{k!}t^{k}$. On définit $M(f,x)(t)=|f(x)|-\sum_{k=1}^{+\infty}|\frac{f^{(k)}(x)}{k!}|t^{k}$
	\end{definition}
	
	\begin{proposition} Si $M(f,x)(\tau)=0$ alors la fonction $f$ ne s'annule pas sur $]x-\tau,x+\tau[$ 
	\end{proposition}

	\begin{algorithm}
		\caption{bisection-exclusion method}
		
		\vspace{2mm}
		
		\textbf{Entrée:} fonction D-finie $f(x)=\sum_{n \geq 0} \mathbb{f}(n)x^{n}$. interval (a,b)
		
		\textbf{Sortie:} $(a_{1},b_{1})\cup (a_{2},b_{2})$
		
		\begin{algorithmic}[1]
			
			\STATE $g(t)=:M(f,\frac{a+b}{2})(t)$
			\STATE c=premier\_zero\_positif(g)
			\STATE Renvoyer($(a,\frac{a+b}{2}-c)\cup (\frac{a+b}{2}+c,b)$)
		\end{algorithmic}
		
	\end{algorithm}
	
	\noindent Ici, on n'a qu'une étape de l'algorithme et l'idée sera bien sûr d'appliquer récursivement cette fonction pour avoir des intervalles de plus en plus précis. L'article de Jakoubsohn donne une discussion plus complète sur la taille des intervalles en fonctions des "clusters" de zéros.\\
	\noindent maj: fonction de majoration fournie dans le module Ore\_algebra.analytic de Marc Mezzarobba\\
	zéropositif(g): le premier zéro sur lequel la fonction g s'annule sur ${R}$
	
	
	\subsection{Séparation des zéros via la méthode de Rouché}
	une autre façon d'exclure des régions de l'espace, maintenant que nous avons des majorations de $\gamma$ est la méthode de Rouché. On introduira la même notation que dans le livre de Dedieu. L'idée ici est d'exclure des intervalles lorsqu'on a trouvé un zéro certifié.\\
	\begin{definition} Soient $\mathbb{E}$ et $\mathbb{F}$ des espaces de Banach et 
		Soit $f:\mathbb{E}\rightarrow \mathbb{F}$ une fonction analytique, notons $\zeta$ un point de $\mathbb{E}$ tel que $f(\zeta)=0$, alors on note
		\[sep(f,\zeta)=\inf_{f(\zeta '=0, \hspace{1mm} \zeta \neq \zeta ')}\left\|\zeta-\zeta '\right\|\] 
	\end{definition}
	
		Le théorème suivant permet de minorer ce nombre de séparations à partir du coefficient $\gamma$\\
	\begin{theorem} Lorsque $f(\zeta)=0$ et que $Df(\zeta)$ est un isomorphisme on a 
		\[sep(f,\zeta) \geq \frac{1}{2\gamma(f,\zeta)}\]
	
	\end{theorem}
	Cependant, dans notre cas, nous n'aurons bien souvent pas la valeur exacte de $\zeta$, il faut donc un théorème plus souple. 
	\begin{theorem} Soit $f: \mathbb{C}^n\rightarrow \mathbb{C}^n$ une fonction analytique, $x_0 \in \mathbb{C}^n$ tel que $Df(x_0)$ soit un isomorphisme. Si $\alpha(f,x_0)<3-2\sqrt{2}$, il existe un et un seul zéro $\zeta$ de f avec
		\[\left\|x_0-\zeta \right\|<\frac{2-\sqrt{2}}{2\gamma(f,x_0)}\]
	\end{theorem}
	
	\noindent On pourra alors certifier le zéro dans un petit intervalle de la forme $[x_0-\frac{2-\sqrt{2}}{2\gamma(f,x_0)},x_0+\frac{2-\sqrt{2}}{2\gamma(f,x_0)}]$
	
\end{document}


